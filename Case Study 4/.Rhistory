knitr::kable(results, caption = "Model Performance Metrics")
caret::confusionMatrix(durdf$acq_preds, durdf$acquisition, positive = '1')
# Load required library
library(caret)
library(dplyr)
# Calculate confusion matrices for each model
conf_log <- caret::confusionMatrix(acq_preds$log_preds, reference = acq_preds$actual, positive = '1')
conf_svm <- caret::confusionMatrix(acq_preds$svm_preds, reference = acq_preds$actual, positive = '1')
conf_rf <- caret::confusionMatrix(acq_preds$rf_preds, reference = acq_preds$actual, positive = '1')
# Extract relevant metrics
results <- data.frame(
Model = c("Logistic Regression", "SVM", "Random Forest"),
Accuracy = c(conf_log$overall["Accuracy"], conf_rf$overall["Accuracy"], conf_svm$overall["Accuracy"]),
Sensitivity = c(conf_log$byClass["Sensitivity"], conf_rf$byClass["Sensitivity"], conf_svm$byClass["Sensitivity"]),
Specificity = c(conf_log$byClass["Specificity"], conf_rf$byClass["Specificity"], conf_svm$byClass["Specificity"])
)
# Round values for better readability
results <- results %>%
mutate(across(Accuracy:Specificity, round, 3))
# Print the results in a clean table
knitr::kable(results, caption = "Model Performance Metrics")
# Load required library
library(caret)
library(dplyr)
# Calculate confusion matrices for each model
conf_log <- caret::confusionMatrix(acq_preds$log_preds, reference = acq_preds$actual, positive = '1')
conf_svm <- caret::confusionMatrix(acq_preds$svm_preds, reference = acq_preds$actual, positive = '1')
conf_rf <- caret::confusionMatrix(acq_preds$rf_preds, reference = acq_preds$actual, positive = '1')
conf_full_data <-caret::confusionMatrix(durdf$acq_preds, durdf$acquisition, positive = '1')
# Extract relevant metrics
results <- data.frame(
Model = c("Logistic Regression", "SVM", "Random Forest", "Random Forrest Full Data Set"),
Accuracy = c(conf_log$overall["Accuracy"], conf_rf$overall["Accuracy"], conf_svm$overall["Accuracy"]),
Sensitivity = c(conf_log$byClass["Sensitivity"], conf_rf$byClass["Sensitivity"], conf_svm$byClass["Sensitivity"]),
Specificity = c(conf_log$byClass["Specificity"], conf_rf$byClass["Specificity"], conf_svm$byClass["Specificity"])
)
# Load required libraries
library(caret)
library(dplyr)
# Calculate confusion matrices for each model
conf_log <- caret::confusionMatrix(acq_preds$log_preds, reference = acq_preds$actual, positive = '1')
conf_svm <- caret::confusionMatrix(acq_preds$svm_preds, reference = acq_preds$actual, positive = '1')
conf_rf <- caret::confusionMatrix(acq_preds$rf_preds, reference = acq_preds$actual, positive = '1')
conf_full_data <- caret::confusionMatrix(durdf$acq_preds, reference = durdf$acquisition, positive = '1')
# Extract relevant metrics
results <- data.frame(
Model = c("Logistic Regression", "SVM", "Random Forest", "Random Forest Full Data Set"),
Accuracy = c(
conf_log$overall["Accuracy"],
conf_svm$overall["Accuracy"],
conf_rf$overall["Accuracy"],
conf_full_data$overall["Accuracy"]
),
Sensitivity = c(
conf_log$byClass["Sensitivity"],
conf_svm$byClass["Sensitivity"],
conf_rf$byClass["Sensitivity"],
conf_full_data$byClass["Sensitivity"]
),
Specificity = c(
conf_log$byClass["Specificity"],
conf_svm$byClass["Specificity"],
conf_rf$byClass["Specificity"],
conf_full_data$byClass["Specificity"]
)
)
# Round values for better readability
results <-
# Load required libraries
library(caret)
library(dplyr)
# Calculate confusion matrices for each model
conf_log <- caret::confusionMatrix(acq_preds$log_preds, reference = acq_preds$actual, positive = '1')
conf_svm <- caret::confusionMatrix(acq_preds$svm_preds, reference = acq_preds$actual, positive = '1')
conf_rf <- caret::confusionMatrix(acq_preds$rf_preds, reference = acq_preds$actual, positive = '1')
conf_full_data <- caret::confusionMatrix(durdf$acq_preds, reference = durdf$acquisition, positive = '1')
# Extract relevant metrics
results <- data.frame(
Model = c("Logistic Regression", "SVM", "Random Forest", "Random Forest Full Data Set"),
Accuracy = c(
conf_log$overall["Accuracy"],
conf_svm$overall["Accuracy"],
conf_rf$overall["Accuracy"],
conf_full_data$overall["Accuracy"]
),
Sensitivity = c(
conf_log$byClass["Sensitivity"],
conf_svm$byClass["Sensitivity"],
conf_rf$byClass["Sensitivity"],
conf_full_data$byClass["Sensitivity"]
),
Specificity = c(
conf_log$byClass["Specificity"],
conf_svm$byClass["Specificity"],
conf_rf$byClass["Specificity"],
conf_full_data$byClass["Specificity"]
)
)
# Round values for better readability
<- results %>%
# Load required libraries
library(caret)
library(dplyr)
# Calculate confusion matrices for each model
conf_log <- caret::confusionMatrix(acq_preds$log_preds, reference = acq_preds$actual, positive = '1')
conf_svm <- caret::confusionMatrix(acq_preds$svm_preds, reference = acq_preds$actual, positive = '1')
conf_rf <- caret::confusionMatrix(acq_preds$rf_preds, reference = acq_preds$actual, positive = '1')
conf_full_data <- caret::confusionMatrix(durdf$acq_preds, reference = durdf$acquisition, positive = '1')
# Extract relevant metrics
results <- data.frame(
Model = c("Logistic Regression", "SVM", "Random Forest", "Random Forest Full Data Set"),
Accuracy = c(
conf_log$overall["Accuracy"],
conf_svm$overall["Accuracy"],
conf_rf$overall["Accuracy"],
conf_full_data$overall["Accuracy"]
),
Sensitivity = c(
conf_log$byClass["Sensitivity"],
conf_svm$byClass["Sensitivity"],
conf_rf$byClass["Sensitivity"],
conf_full_data$byClass["Sensitivity"]
),
Specificity = c(
conf_log$byClass["Specificity"],
conf_svm$byClass["Specificity"],
conf_rf$byClass["Specificity"],
conf_full_data$byClass["Specificity"]
)
)
# Round values for better readability
results<- results %>%
mutate(across(Accuracy:Specificity, round, 3))
# Print the results in a clean table
knitr::kable(results, caption = "Model Performance Metrics")
acq_raw_prob <- predict(rffit1, newdata = select(rawdf, names(acq_test1)))$predicted[,2]
durdf <- rawdf %>% bind_cols(preds = acq_raw_prob) %>% mutate(
acq_preds = as.factor(ifelse(preds >= 0.5, 1, 0))
)
pacman::p_load(survival, randomForestSRC, MASS, SMCRM, tidyverse, here, skimr, corrplot, rpart, e1071)
data(acquisitionRetention)
rawdf <- acquisitionRetention
rawdf <- rawdf %>% mutate_at(c('industry', 'acquisition'), as.factor)
rawdf %>% select_if(is.numeric) %>% cor() %>% corrplot(method = 'number', number.cex = 0.6)
# Survival Plot
surv <- data.frame(duration = seq(0,max(rawdf$duration), by = 1))
surv1 <- surv %>%
group_by(duration) %>%
mutate(dist = sum(rawdf$duration > duration))
#greater than removes all of the results where duration == 0, i.e. those not acquired
surv1 %>% ggplot(aes(x = duration, y = dist)) +
geom_line() +
scale_y_continuous(name = 'Count', sec.axis = sec_axis(transform = ~./max(surv1$dist), name = 'Distribution'))
sum(rawdf[which(rawdf$acquisition == 0),]$duration)
rawdf %>% select(ends_with('sq')) %>% names()
set.seed(321)
acq_part <- sample(nrow(rawdf),0.8*nrow(rawdf),replace = F)
acq_train1 <- rawdf[acq_part,]
acq_test1 <- rawdf[-acq_part,]
acq_train1 <- acq_train1 %>% select(-c(profit, duration, ret_exp, ret_exp_sq, freq, freq_sq, crossbuy, sow))
acq_test1 <- acq_test1 %>% select(-c(profit, duration, ret_exp, ret_exp_sq, freq, freq_sq, crossbuy, sow))
set.seed(321)
logfit1 <- step(glm(acquisition ~ . -customer,
data = acq_train1, family = binomial),
direction = "backward", trace = 0)
summary(logfit1)
car::vif(logfit1)
set.seed(321)
logfit2 <- step(glm(acquisition ~ . -acq_exp_sq -customer,
data = acq_train1, family = binomial),
direction = "backward", trace = 0)
summary(logfit2)
car::vif(logfit2)
plot(logfit2)
acq_preds <- data.frame(actual = acq_test1$acquisition,
log_preds = predict(logfit2, acq_test1, type = 'response')) %>% mutate(
log_preds = as.factor(ifelse(log_preds >= 0.5, 1, 0))
)
set.seed(321)
svmfit1 <- svm(acquisition ~ acq_exp + industry + revenue + employees,
data = acq_train1,
type = "C-classification",
kernel = "radial",
cost = 1, #<--we can adjust this if needed
scale = TRUE
)
summary(svmfit1)
svm_preds <- predict(svmfit1, acq_test1)
acq_preds$svm_preds <- svm_preds
svm_cm <- caret::confusionMatrix(acq_preds$svm_preds,
reference = acq_preds$actual,
positive = '1')
print(svm_cm)
set.seed(321)
tune.out <- tune(svm,
acquisition ~ acq_exp + industry + revenue + employees,
data = acq_train1,
kernel = "radial",
ranges = list(gamma = seq(.01,.1, by = .01), cost = seq(.1, 1, by = .1)))
svmfit2 <- tune.out$best.model
summary(svmfit2)
svm_preds2 <- predict(svmfit2, acq_test1)
acq_preds$svm_preds2 <- svm_preds2
svm_cm2 <- caret::confusionMatrix(acq_preds$svm_preds2,
reference = acq_preds$actual,
positive = '1')
print(svm_cm2)
set.seed(321)
dtfit1 <- rpart(acquisition ~ . -customer, data = acq_train1)
summary(dtfit1)
rattle::fancyRpartPlot(dtfit1, sub = '')
acq_preds$dt_preds <- predict(dtfit1, acq_test1)
set.seed(321)
rffit1 <- rfsrc(acquisition ~ acq_exp + industry + revenue + employees,
data = acq_train1,
importance = TRUE,
ntree = 100)
rffit1
rffit1$importance
data.frame(importance = rffit1$importance[,3]) %>%
tibble::rownames_to_column(var = "variable") %>%
ggplot(aes(x = reorder(variable,importance), y = importance)) +
geom_bar(stat = "identity", fill = "orange", color = "black")+
coord_flip() +
labs(x = "Variables", y = "Variable importance")
acq_mindepth <- max.subtree(rffit1, sub.order = T)
print(round(acq_mindepth$order, 3)[,1])
find.interaction(rffit1, method = 'vimp', importance = 'permute')
acq_test_prob <- predict(rffit1, newdata = acq_test1)$predicted[,2]
acq_preds$rf_preds <- acq_test_prob
acq_preds <- acq_preds %>% mutate(rf_preds = as.factor(ifelse(rf_preds >= 0.5, 1, 0)))
#caret::confusionMatrix(acq_preds$log_preds, reference = acq_preds$actual, positive = '1')
#caret::confusionMatrix(acq_preds$svm_preds, reference = acq_preds$actual, positive = '1')
# Load required libraries
library(caret)
library(dplyr)
# Calculate confusion matrices for each model
conf_log <- caret::confusionMatrix(acq_preds$log_preds, reference = acq_preds$actual, positive = '1')
conf_svm <- caret::confusionMatrix(acq_preds$svm_preds, reference = acq_preds$actual, positive = '1')
conf_rf <- caret::confusionMatrix(acq_preds$rf_preds, reference = acq_preds$actual, positive = '1')
conf_full_data <- caret::confusionMatrix(durdf$acq_preds, reference = durdf$acquisition, positive = '1')
acq_raw_prob <- predict(rffit1, newdata = select(rawdf, names(acq_test1)))$predicted[,2]
durdf <- rawdf %>% bind_cols(preds = acq_raw_prob) %>% mutate(
acq_preds = as.factor(ifelse(preds >= 0.5, 1, 0))
)
# Load required libraries
library(caret)
library(dplyr)
# Calculate confusion matrices for each model
conf_log <- caret::confusionMatrix(acq_preds$log_preds, reference = acq_preds$actual, positive = '1')
conf_svm <- caret::confusionMatrix(acq_preds$svm_preds, reference = acq_preds$actual, positive = '1')
conf_rf <- caret::confusionMatrix(acq_preds$rf_preds, reference = acq_preds$actual, positive = '1')
conf_full_data <- caret::confusionMatrix(durdf$acq_preds, reference = durdf$acquisition, positive = '1')
# Extract relevant metrics
results <- data.frame(
Model = c("Logistic Regression", "SVM", "Random Forest", "Random Forest Full Data Set"),
Accuracy = c(
conf_log$overall["Accuracy"],
conf_svm$overall["Accuracy"],
conf_rf$overall["Accuracy"],
conf_full_data$overall["Accuracy"]
),
Sensitivity = c(
conf_log$byClass["Sensitivity"],
conf_svm$byClass["Sensitivity"],
conf_rf$byClass["Sensitivity"],
conf_full_data$byClass["Sensitivity"]
),
Specificity = c(
conf_log$byClass["Specificity"],
conf_svm$byClass["Specificity"],
conf_rf$byClass["Specificity"],
conf_full_data$byClass["Specificity"]
)
)
# Round values for better readability
results<- results %>%
mutate(across(Accuracy:Specificity, round, 3))
# Print the results in a clean table
knitr::kable(results, caption = "Model Performance Metrics")
# Calculate metrics
rmse <- sqrt(mean((dur_preds$actual - dur_preds$preds)^2))
data.frame(importance = rfdur$importance) %>%
tibble::rownames_to_column(var = "variable") %>%
ggplot(aes(x = reorder(variable,importance), y = importance)) +
geom_bar(stat = "identity", fill = "orange", color = "black")+
coord_flip() +
labs(x = "Variables", y = "Variable importance")
pacman::p_load(survival, randomForestSRC, MASS, SMCRM, tidyverse, here, skimr, corrplot, rpart, e1071,caret,dplyr)
data(acquisitionRetention)
rawdf <- acquisitionRetention
rawdf <- rawdf %>% mutate_at(c('industry', 'acquisition'), as.factor)
rawdf %>% select_if(is.numeric) %>% cor() %>% corrplot(method = 'number', number.cex = 0.6)
# Survival Plot
surv <- data.frame(duration = seq(0,max(rawdf$duration), by = 1))
surv1 <- surv %>%
group_by(duration) %>%
mutate(dist = sum(rawdf$duration > duration))
#greater than removes all of the results where duration == 0, i.e. those not acquired
surv1 %>% ggplot(aes(x = duration, y = dist)) +
geom_line() +
scale_y_continuous(name = 'Count', sec.axis = sec_axis(transform = ~./max(surv1$dist), name = 'Distribution'))
sum(rawdf[which(rawdf$acquisition == 0),]$duration)
rawdf %>% select(ends_with('sq')) %>% names()
set.seed(321)
acq_part <- sample(nrow(rawdf),0.8*nrow(rawdf),replace = F)
acq_train1 <- rawdf[acq_part,]
acq_test1 <- rawdf[-acq_part,]
acq_train1 <- acq_train1 %>% select(-c(profit, duration, ret_exp, ret_exp_sq, freq, freq_sq, crossbuy, sow))
acq_test1 <- acq_test1 %>% select(-c(profit, duration, ret_exp, ret_exp_sq, freq, freq_sq, crossbuy, sow))
set.seed(321)
logfit1 <- step(glm(acquisition ~ . -customer,
data = acq_train1, family = binomial),
direction = "backward", trace = 0)
summary(logfit1)
car::vif(logfit1)
set.seed(321)
logfit2 <- step(glm(acquisition ~ . -acq_exp_sq -customer,
data = acq_train1, family = binomial),
direction = "backward", trace = 0)
summary(logfit2)
car::vif(logfit2)
plot(logfit2)
acq_preds <- data.frame(actual = acq_test1$acquisition,
log_preds = predict(logfit2, acq_test1, type = 'response')) %>% mutate(
log_preds = as.factor(ifelse(log_preds >= 0.5, 1, 0))
)
set.seed(321)
svmfit1 <- svm(acquisition ~ acq_exp + industry + revenue + employees,
data = acq_train1,
type = "C-classification",
kernel = "radial",
cost = 1, #<--we can adjust this if needed
scale = TRUE
)
summary(svmfit1)
svm_preds <- predict(svmfit1, acq_test1)
acq_preds$svm_preds <- svm_preds
svm_cm <- caret::confusionMatrix(acq_preds$svm_preds,
reference = acq_preds$actual,
positive = '1')
print(svm_cm)
set.seed(321)
tune.out <- tune(svm,
acquisition ~ acq_exp + industry + revenue + employees,
data = acq_train1,
kernel = "radial",
ranges = list(gamma = seq(.01,.1, by = .01), cost = seq(.1, 1, by = .1)))
svmfit2 <- tune.out$best.model
summary(svmfit2)
svm_preds2 <- predict(svmfit2, acq_test1)
acq_preds$svm_preds2 <- svm_preds2
svm_cm2 <- caret::confusionMatrix(acq_preds$svm_preds2,
reference = acq_preds$actual,
positive = '1')
print(svm_cm2)
set.seed(321)
dtfit1 <- rpart(acquisition ~ . -customer, data = acq_train1)
summary(dtfit1)
rattle::fancyRpartPlot(dtfit1, sub = '')
acq_preds$dt_preds <- predict(dtfit1, acq_test1)
set.seed(321)
rffit1 <- rfsrc(acquisition ~ acq_exp + industry + revenue + employees,
data = acq_train1,
importance = TRUE,
ntree = 100)
rffit1
rffit1$importance
data.frame(importance = rffit1$importance[,3]) %>%
tibble::rownames_to_column(var = "variable") %>%
ggplot(aes(x = reorder(variable,importance), y = importance)) +
geom_bar(stat = "identity", fill = "orange", color = "black")+
coord_flip() +
labs(x = "Variables", y = "Variable importance")
acq_mindepth <- max.subtree(rffit1, sub.order = T)
print(round(acq_mindepth$order, 3)[,1])
find.interaction(rffit1, method = 'vimp', importance = 'permute')
acq_test_prob <- predict(rffit1, newdata = acq_test1)$predicted[,2]
acq_preds$rf_preds <- acq_test_prob
acq_preds <- acq_preds %>% mutate(rf_preds = as.factor(ifelse(rf_preds >= 0.5, 1, 0)))
acq_raw_prob <- predict(rffit1, newdata = select(rawdf, names(acq_test1)))$predicted[,2]
durdf <- rawdf %>% bind_cols(preds = acq_raw_prob) %>% mutate(
acq_preds = as.factor(ifelse(preds >= 0.5, 1, 0))
)
# Load required libraries
# Calculate confusion matrices for each model
conf_log <- caret::confusionMatrix(acq_preds$log_preds, reference = acq_preds$actual, positive = '1')
conf_svm <- caret::confusionMatrix(acq_preds$svm_preds, reference = acq_preds$actual, positive = '1')
conf_rf <- caret::confusionMatrix(acq_preds$rf_preds, reference = acq_preds$actual, positive = '1')
conf_full_data <- caret::confusionMatrix(durdf$acq_preds, reference = durdf$acquisition, positive = '1')
# Extract relevant metrics
results <- data.frame(
Model = c("Logistic Regression", "SVM", "Random Forest", "Random Forest Full Data Set"),
Accuracy = c(
conf_log$overall["Accuracy"],
conf_svm$overall["Accuracy"],
conf_rf$overall["Accuracy"],
conf_full_data$overall["Accuracy"]
),
Sensitivity = c(
conf_log$byClass["Sensitivity"],
conf_svm$byClass["Sensitivity"],
conf_rf$byClass["Sensitivity"],
conf_full_data$byClass["Sensitivity"]
),
Specificity = c(
conf_log$byClass["Specificity"],
conf_svm$byClass["Specificity"],
conf_rf$byClass["Specificity"],
conf_full_data$byClass["Specificity"]
)
)
# Round values for better readability
results<- results %>%
mutate(across(Accuracy:Specificity, round, 3))
# Print the results in a clean table
knitr::kable(results, caption = "Model Performance Metrics")
durdf1 <- durdf %>% filter(acq_preds == 1) %>% select(-acquisition)
set.seed(321)
dur_part <- sample(nrow(durdf1),0.8*nrow(durdf1),replace = F)
dur_train1 <- durdf1[dur_part,]
dur_test1 <- durdf1[-dur_part,]
names(dur_train1)
set.seed(321)
rfdur <- rfsrc(duration ~ profit + acq_exp + ret_exp + freq + crossbuy + sow + industry + revenue + employees, #-customer -acq_exp_sq -ret_exp_sq -freq_sq,
data = dur_train1,
importance = TRUE,
ntree = 500)
rfdur
data.frame(importance = rfdur$importance) %>%
tibble::rownames_to_column(var = "variable") %>%
ggplot(aes(x = reorder(variable,importance), y = importance)) +
geom_bar(stat = "identity", fill = "orange", color = "black")+
coord_flip() +
labs(x = "Variables", y = "Variable importance")
#     theme_nice
rfdur$importance
dur_mindepth <- max.subtree(rfdur,
sub.order = TRUE)
print(round(dur_mindepth$order, 3)[,1])
dur_mindepth$sub.order
find.interaction(rfdur,
method = "vimp",
importance = "permute")
# regression with linear specification
dur_reg_lin <- lm(duration ~  .-acq_exp_sq -ret_exp_sq -freq_sq -customer, data = select(dur_train1, -acq_preds))
dur_reg_exp <- lm(duration ~  .-customer, data = select(dur_train1, -acq_preds))
summary(dur_reg_lin)
summary(dur_reg_exp)
min(rfdur$xvar$ret_exp)
max(rfdur$xvar$ret_exp)
ret_exp_seq = seq(0,1100,20)
min(rfdur$xvar$freq)
max(rfdur$xvar$freq)
freq_seq <- seq(1,21,1)
min(rfdur$xvar$acq_exp)
max(rfdur$xvar$acq_exp)
acq_exp_seq <- seq(140,880,20)
retx_me <- randomForestSRC::partial(rfdur,
partial.xvar = "ret_exp",
partial.values = ret_exp_seq)
retx_me_means <- retx_me$regrOutput$duration %>% colMeans()
retx_me_df <-
data.frame(pred_duration = retx_me_means, ret_exp_seq = ret_exp_seq)
ggplot(retx_me_df, aes(x = ret_exp_seq, y = pred_duration)) +
geom_point(shape = 21, color = "purple", size = 2, stroke = 1.2)+
geom_smooth(method = "lm", formula = y ~ poly(x,6), se = FALSE, color = "black")+ # try with other values
labs(x = "Retention Expenditures in $", y = "Predicted duration", title = 'Partial Dependence Plot: Retention Expenditure') +
scale_x_continuous(breaks = seq(0,1200,120))
freq_me <- randomForestSRC::partial(rfdur,
partial.xvar = "freq",
partial.values = freq_seq)
freq_me_means <- freq_me$regrOutput$duration %>% colMeans()
freq_me_df <-
data.frame(pred_duration = freq_me_means, freq_seq = freq_seq)
ggplot(freq_me_df, aes(x = freq_seq, y = pred_duration)) +
geom_point(shape = 21, color = "purple", size = 2, stroke = 1.2)+
geom_smooth(method = "lm", formula = y ~ poly(x,5), se = FALSE, color = "black")+ # try with other values
labs(x = "Frequency of Purchases", y = "Predicted duration", title = 'Partial Dependence Plot: Frequency') +
scale_x_continuous(breaks = seq(0,21,3))
acqx_me <- randomForestSRC::partial(rfdur,
partial.xvar = "acq_exp",
partial.values = acq_exp_seq)
acqx_me_means <- acqx_me$regrOutput$duration %>% colMeans()
acqx_me_df <-
data.frame(pred_duration = acqx_me_means, acq_exp_seq = acq_exp_seq)
ggplot(acqx_me_df, aes(x = acq_exp_seq, y = pred_duration)) +
geom_point(shape = 21, color = "purple", size = 2, stroke = 1.2)+
geom_smooth(method = "lm", formula = y ~ poly(x,9), se = FALSE, color = "black")+ # try with other values
labs(x = "Acquisition Expenditures in $", y = "Predicted duration", title = 'Partial Dependence Plot: Acquisiton Expenditure') +
scale_x_continuous(breaks = seq(150,1000,150))
dur_preds <- data.frame(actual = dur_test1$duration,
preds = predict(rfdur, dur_test1)$predicted)
# Calculate metrics
rmse <- sqrt(mean((dur_preds$actual - dur_preds$preds)^2))
mae <- mean(abs(dur_preds$actual - dur_preds$preds))
me <- mean(dur_preds$actual - dur_preds$preds)
mape <- mean(abs(dur_preds$actual - dur_preds$preds) * 100 / (dur_preds$actual + 0.1))
# Create a data frame for results
results <- data.frame(
Metric = c("RMSE", "MAE", "ME", "MAPE"),
Value = c(rmse, mae, me, mape)
)
# Round values for better readability
results <- results %>%
mutate(Value = round(Value, 3))
# Print the results in a clean table
knitr::kable(results, caption = "Error Metrics for Predictions")
#print(paste('rmse', sqrt(mean((dur_preds$actual - dur_preds$preds)^2))))
#print(paste('mae', mean(abs(dur_preds$actual - dur_preds$preds))))
#print(paste('me', mean(dur_preds$actual - dur_preds$preds)))
#print(paste('mape', mean(abs(dur_preds$actual - dur_preds$preds)*100/(dur_preds$actual + .1))))
