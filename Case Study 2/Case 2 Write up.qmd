---
title: "DA 6813 Case Study 2 "
author: "Will Hytlin, Holly Millazo and Tim Harrison"
date: today
format: 
  html:
    theme: lumen # or flatly, lumen, united, etc.
    toc: true # Adds a table of contents
    toc-depth: 2 # Depth of table of contents
    toc-location: left
    code-fold: true # Folds code by default
    number-sections: true # Numbered sections
    df-print: paged # Nice table format
    fig-align: center # Centers figures
    fig-width: 6 # Adjust figure size
    fig-height: 4
    reference-location: section # Valid value for citation references
---

```{r}
pacman::p_load(MASS, tidyverse, e1071, here, readxl, skimr, corrplot, patchwork)

raw_train <- read_xlsx(here('Case Study 2', 'BBBC-Train.xlsx'))
raw_test <- read_xlsx(here('Case Study 2', 'BBBC-Test.xlsx'))
```


# Executive Summary

<!-- 
Purpose: Provide a concise overview of the key findings and results of the analysis.
Instructions: Summarize the performance of the models (e.g., random forest, decision tree, logistic regression). Highlight the best-performing model and key insights regarding customer acquisition. Avoid technical details, focusing on high-level conclusions that decision-makers would care about.
-->

This study aimed to develop a predictive model for the Bookbinders Book Club (BBBC) to determine which customers were likely to purchase The Art History of Florence following a direct mail campaign. The analysis was conducted on a dataset containing customer demographics, purchasing behavior, and preferences for different book genres. The key variables considered included gender, amount spent on BBBC books, frequency of purchases, and the number of specific genres purchased (e.g., children's books, cookbooks, art books).

Three different modeling techniques were evaluated: linear regression, logistic regression, and support vector machines (SVM). Given that the dependent variable was binary (purchase or no purchase), linear regression was found to be unsuitable as it attempts to predict a continuous outcome rather than a classification, leading to misleading results. Logistic regression and SVM were thus compared for their predictive performance on the unbalanced and balanced datasets.

Initial logistic regression results on the unbalanced data showed moderate accuracy (65.61%) and balanced accuracy (71.84%) but low sensitivity (17.78%). However, balancing the dataset improved sensitivity and overall predictive performance. Similarly, the SVM model initially underperformed due to the unbalanced nature of the data but saw significant improvement after balancing, with a final accuracy of 73.77%, sensitivity of 65.69%, and specificity of 81.86%.

A key insight was that improving sensitivity, even at the cost of specificity, was critical for maximizing revenue. By lowering the decision threshold in the logistic regression model, the sensitivity increased, resulting in a higher proportion of correctly identified purchasers. Despite the reduction in specificity, the model captured a larger number of likely buyers, ultimately leading to greater profit potential compared to a naïve approach.

The profitability analysis showed that while the logistic regression and SVM models performed better than random guessing, there is still room for improvement. Adjusting model parameters such as the decision threshold and considering alternative models, like LDA, could further enhance profitability by improving the balance between targeting the right customers and controlling campaign costs.

In conclusion, while the logistic regression model with an optimized threshold delivered the best balance of revenue and costs, further model refinement and variable tuning are necessary to maximize campaign effectiveness.

# Problem Statement

<!-- 
Purpose: Explain the problem that the study aims to solve.
Instructions: Clearly define the task at hand, which in this case is predicting customer acquisition. Outline the objectives and what solving this problem would mean for the company.
-->

The task of this case study is to develop a predictive model to classify whether customers of the Bookbinders Book Club (BBBC) will purchase The Art History of Florence following a direct mail marketing campaign. The campaign involved sending a specially produced brochure to selected customers in Pennsylvania, New York, and Ohio, aiming to assess the likelihood of each customer making a purchase ('yes') or not ('no').

The goal is to accurately predict the likelihood of a customer purchasing the book based on various input variables, including demographic factors (such as gender) and past purchasing behaviors, including the total amount spent on BBBC books, the frequency of past purchases, and preferences for different book genres (such as children's books, cookbooks, do-it-yourself, and art books). These factors are believed to significantly influence the decision to purchase the featured book.

The primary objective is to build a classification model that can accurately predict customer purchases, enabling BBBC to target its marketing efforts more effectively. By identifying the most likely purchasers, BBBC can optimize resource allocation, reduce unnecessary mailer costs, and improve the overall conversion rate of its marketing campaigns.

# Additional Sources
<!-- 
Purpose: Reference relevant literature or sources to support the analysis.
Instructions: Provide citations and key insights from sources relevant to the case study or the models used.
-->

Aldelemy, A., & Abd-Alhameed, R. A. (2023). Binary classification of customer’s online purchasing behavior using machine learning. Journal of Techniques, 5(2), 163–186. https://doi.org/10.51173/jt.v5i2.1226

This reference highlights the strong performance of logistic regression compared to other models, which supports our conclusion where logistic regression ultimately outperformed other methods


# Methodology

<!-- 
Purpose: Describe the methods and processes used to conduct the analysis.
Instructions: Detail the models used and the reasoning behind them. Include specifics such as hyperparameter tuning, data splitting, and assumptions of each model.
-->

The analysis began with data preparation, where the dataset of 12 variables, both categorical and numeric, was cleaned and transformed. The categorical variable Gender was converted to a binary factor, and the target variable, Choice, which indicated whether a customer purchased The Art History of Florence, was transformed into a binary indicator (1 for purchase, 0 for no purchase). Variables representing different genres of books purchased, such as P_Child, P_Youth, P_Cook, P_DIY, and P_Art, were retained as numeric variables reflecting the number of books purchased in each category.

Exploratory data analysis (EDA) was conducted to examine the distribution and relationships within the data. Histograms and box plots were generated to visualize the distribution of numeric variables such as Amount_purchased, Frequency, and Last_purchase based on the outcome variable Choice. Bar plots were used to explore the frequency of categorical variables like Gender. A correlation matrix was constructed to identify relationships among numeric variables and to detect potential multicollinearity issues.

The dataset initially provided was two pre-split sets: one for training and one for testing. However, these datasets were later combined for exploratory data analysis (EDA), correlation analysis, and visualization. The training set contained 80% of the data, and the test set comprised 20%. The combination allowed for comprehensive analysis while ensuring that model evaluation was still conducted on unseen data.

Several modeling techniques were explored, including logistic regression, linear discriminant analysis (LDA), and support vector machines (SVM). Logistic regression was selected as the primary technique due to its suitability for binary classification. A stepwise backward selection method, based on the Akaike Information Criterion (AIC), was used to remove insignificant variables and select the most relevant predictors. To address potential multicollinearity, variables with high variance inflation factor (VIF) values were removed.

Model performance was evaluated using accuracy, sensitivity, and specificity, with results summarized in a confusion matrix. To further address class imbalance in the test set, the decision threshold for classifying customers was adjusted to optimize model performance. By iterating over different threshold values, an optimal cutoff was determined that balanced sensitivity and specificity, enhancing the model’s ability to predict both purchasers and non-purchasers effectively.


Finally, a profitability analysis was conducted to evaluate the financial impact of the model. The cost of sending mailers and the revenue from book purchases were calculated to determine the overall profit for each modeling approach.

# Data

<!-- 
Purpose: Explain the data used in the analysis and any preprocessing steps.
Instructions: Provide an overview of the dataset, variables, and any cleaning steps or transformations made. Mention variables used or excluded and why.
-->

The dataset used for this analysis contains a total of 12 variables across both training and testing sets. These variables represent customer demographics, purchasing behavior, and preferences for various book genres at the Bookbinders Book Club (BBBC). The key target variable is Choice, which indicates whether a customer purchased The Art History of Florence. The data consists of both categorical and numeric variables.

```{r, echo=FALSE}
skim(combined)
```

A check for missing values was performed (anyNA()), and no missing data was detected, so no further imputation or cleaning steps were necessary in that regard.

We removed Observation column, converted categorical variables (Choice and Gender) into factors, and combined the training and testing datasets for further analysis or visualization.

During our exploratory data analysis (EDA), various visualizations were used to examine the distributions and relationships within the dataset. A correlation plot was created to assess the relationships among numeric variables such as Amount_purchased, Frequency, Last_purchase, First_purchase, and the number of different types of books purchased (e.g., P_Child, P_Youth, P_Cook, P_DIY, P_Art). This helped to identify any strong correlations or multicollinearity between the numeric features.

```{r}
combined %>% select_if(is.numeric) %>% cor() %>% corrplot(method = 'number')
```

Bar plots were used to explore the distribution of categorical variables such as Gender and Choice (purchase or non-purchase). For example, a bar plot was generated to visualize the relationship between gender and purchase behavior, displaying the frequency of purchases and non-purchases among males and females. These visualizations provided insights into the key factors that might influence the likelihood of a customer purchasing a book.


# Findings

<!-- 
Purpose: Present the results of the analysis.
Instructions: Report accuracy rates and compare the performance of models. Discuss significant variables or interactions discovered.
-->

Upon initial assesment of our SVM on balanced data, the sensitivity of the model was relatively low, but this wasn't a significant issue given our objective. The model predicted that 160 out of 408 observations would likely purchase the book. To ensure the integrity of our model and data, we applied appropriate transformations and balanced the responses in both the training and test sets. However, since we have no knowledge of the distribution of responses in the actual mailing list audience, we cannot assume that it will be balanced. Therefore, it was important to validate our model's performance on an unbalanced dataset to ensure it remained effective in real-world scenarios, where the distribution of purchasers and non-purchasers may differ.

After applying the SVM model to the original unbalanced test dataset, the sensitivity and specificity metrics remained consistent with those observed in the balanced dataset. This outcome is logical because the distribution between positive (purchasers) and negative (non-purchasers) cases only affects the overall prevalence, not the fundamental calculations of sensitivity and specificity. Each metric remained robust regardless of changes in class distribution because they were calculated independently within each class. As a result, we could apply these performance metrics to a hypothetical, unbalanced dataset of random customers.

```{r}
svmpredict_imbal <- predict(svmfit_bal, test1, type = 'response')
SVM_CM_imbal <- caret::confusionMatrix(svmpredict_imbal, test1$Choice, positive = '1')
SVM_CM_imbal
#caret::confusionMatrix(svmpredict_imbal, test1$Choice, positive = '1')
```

However, our overall analysis revealed that the logistic regression outperformed the other models in terms of overall prediction accuracy, particularly after the decision threshold was optimized. By adjusting the threshold, the model's sensitivity significantly improved, allowing it to correctly identify a larger number of customers who were likely to purchase the featured book. While the support vector machine (SVM) model initially demonstrated poor sensitivity due to the unbalanced nature of the dataset, its performance improved once the data was balanced. The SVM model showed strong specificity, meaning it effectively reduced false positives, but this came at the cost of lower sensitivity. Linear discriminant analysis (LDA) performed similarly to logistic regression but did not achieve the same level of sensitivity as the threshold-optimized logistic model.

How we gathered these findings were by first using data from the training and test sets, the proportion of people who are expected to purchase the book out of the 50,000 people in the mailing audience is calculated then storing this estimate in a column called 'newcnt'. We then used the model to estimate how many of the individuals in both the purchasing and non-purchasing groups would be predicted to buy the book called 'est_targets'.

The cost of sending mailers was calculated by multiplying the number of predicted buyers ("est_targets") by $0.65 (the cost of each mailer) and called 'mailercst'.

For those who are predicted to buy the book, the total cost of the books and overhead (calculated as $15 x 1.45) was estimated. These costs are only applied to those predicted to purchase the book ("newcnt" where Choice == 1), "purchcst" variable. The total revenue from book sales is calculated by multiplying the number of predicted buyers by $31.95 (the price of the book). 'Revenue' is only generated when Choice == 1, and 'Profit' is calculated by subtracting both the mailer and book purchase costs from the total revenue.

We see by summing up the values in the "profit" we get a total expected profit from the mailer campaign. We see the comparison results here:

```{r}
data.frame(Model = c('Naive', 'LDA', 'Logit', 'SVM'), 
  Profit = c(
    sum(naive_table$profit),
    sum(summary_table_lda$profit),
    sum(summary_table_log$profit),
    sum(summary_table_svm$profit)
    )
  )
```


One of the primary challenges in the dataset was the inherent class imbalance, with significantly more non-purchasers than purchasers. To address this, the dataset was balanced by oversampling the minority class (purchasers), which improved the performance of both logistic regression and SVM models, particularly in terms of sensitivity. Even when tested on the original unbalanced dataset, the sensitivity and specificity metrics for both models remained consistent, indicating that the models were robust against changes in class distribution.

The analysis also highlighted a trade-off between sensitivity and specificity. Improving sensitivity was crucial for identifying a greater proportion of potential purchasers, which is the primary goal of the direct mail campaign. However, this improvement came at the cost of specificity, meaning that some mailers would be sent to non-purchasers, resulting in false positives. Despite this, the increased sensitivity was considered an acceptable trade-off, as the cost of sending mailers to non-purchasers is relatively low compared to the revenue generated from correctly identified purchasers.

Finally, the profitability analysis showed that the logistic regression model with an optimized threshold provided the best balance between sensitivity and specificity, leading to the highest potential profit for the campaign. The SVM model, while strong in terms of specificity, identified fewer purchasers overall, limiting its potential revenue generation. This analysis demonstrated that balancing the dataset and fine-tuning the decision threshold were critical steps in maximizing the effectiveness and profitability of the direct mail campaign.

```{r}
thresh %>% ggplot(aes(x = threshold, y = profit)) +
  geom_line() +
  geom_point() +
  annotate('text', x = thresh[which(thresh$profit == max(thresh$profit)),]$threshold, y = thresh[which(thresh$profit == max(thresh$profit)),]$profit + 1800, label = paste0('Max Profit: $', thresh[which(thresh$profit == max(thresh$profit)),]$profit), size = 3) +
  annotate('point', x = thresh[which(thresh$profit == max(thresh$profit)),]$threshold, y = thresh[which(thresh$profit == max(thresh$profit)),]$profit, color = 'green', shape = 'diamond', size = 3) +
  theme_minimal() +
  ggtitle('Book Sales Profit Changes by Model Probability Threshold') +
  xlab('Logistic Model Probability Threshold') +
  ylab('Profit from Book Sales')

  
thresh[which(thresh$profit == max(thresh$profit)),]
```
In a similar study on "Binary Classification of Customer’s Online Purchasing Behavior Using Machine Learning", the strength of logistic regression compared to other models was also found: "A comparative study of ten classifiers is presented in [18]. Their accuracy indicator, i.e., the area under the curve (AUC), highlighted logistic regression as the best classifier. Naive Bayes, neural network, and support vector machine classifiers followed as runners-up, while decision tree-based classifiers tended to underperform."

# Conclusion

<!-- 
Purpose: Summarize the key takeaways and provide actionable recommendations.
Instructions: Conclude with the most important findings and offer suggestions for further analysis or improvements.
-->

In conclusion, the logistic regression model ultimately performed the best in predicting customer purchases for the mailer campaign. 

By iterating through different decision thresholds, we identified the optimal threshold that maximized profit, adjusting the threshold down to 0.2 for the logistic model. While the naive method yielded higher revenue by reaching all potential buyers, using a predictive model like logistic regression or LDA with an optimized threshold helped balance mailer costs and capture more actual purchasers. This approach resulted in higher overall profitability by efficiently targeting customers most likely to buy the book, demonstrating that a carefully tuned predictive model provides a more cost-effective solution than the naive approach.





# Appendix

<!-- 
Purpose: Include supplementary material or detailed technical results.
Instructions: Provide code snippets, detailed model output, and data summaries.
-->


```{r setup}
pacman::p_load(MASS, tidyverse, e1071, here, readxl, skimr, corrplot, patchwork)

raw_train <- read_xlsx(here('Case Study 2', 'BBBC-Train.xlsx'))
raw_test <- read_xlsx(here('Case Study 2', 'BBBC-Test.xlsx'))
```

```{r}
str(raw_train)
```
```{r}
anyNA(raw_train)
```

```{r}
skim(raw_train)
```

```{r}
train1 <- raw_train %>% 
  select(-Observation) %>% 
  mutate(
    Choice = as.factor(Choice),
    Gender = as.factor(Gender)
    )
test1 <- raw_test %>% 
  select(-Observation) %>%
  mutate(
    Choice = as.factor(Choice),
    Gender = as.factor(Gender)
    )

combined <- rbind(train1, test1)
```

```{r}
combined %>% select_if(is.numeric) %>% cor() %>% corrplot(method = 'number')
```

```{r}
combined %>% mutate(
  Gender = ifelse(Gender == 0, 'Female', 'Male'),
  Choice = ifelse(Choice == 0, 'Non-purchase', 'purchase')
  ) %>% 
  ggplot(aes(x = Gender, fill = Choice)) +
  geom_bar() + ggtitle('Gender vs Purchase')
```

```{r}
combox <- lapply(colnames(select_if(combined, is.numeric)),
       function(col) {
        ggplot(combined,
                aes(y = .data[[col]], x = .data$Choice)) + geom_boxplot() + ggtitle(col)
       }
)

combox[[1]] + combox[[2]]
combox[[3]] + combox[[4]]
```

Boxplots are a bad visual for the variables starting with "P_". Contingency table below helps, but maybe grouped bar charts for those too? I have those below, let me know what yall think.

```{r}
combbar <- lapply(colnames(select_if(combined, startsWith(names(combined), 'P_'))),
       function(col) {
        ggplot(combined,
                aes(x = .data[[col]], fill = .data$Choice)) + geom_bar(position = 'dodge') + 
           ggtitle(col) + 
           theme(legend.position = c(0.8,0.8), legend.background = element_blank())
       }
)

combbar[[1]] + combbar[[2]]
combbar[[3]] + combbar[[4]]
combbar[[5]]
```


```{r}
combined %>% group_by(Choice, P_Art) %>% 
  summarize(cnt = n()) %>% pivot_wider(id_cols = P_Art, names_from = Choice, values_from = cnt)
```

### Linear Regression

```{r}
#Creating data frames that don't make Choice a factor only to run Linear Regression. This is done to show it isn't the appropriate model.
linregtrain1 <- raw_train %>% 
  select(-Observation) %>% 
  mutate(
    Gender = as.factor(Gender)
    )
linregtest1 <- raw_test %>% 
  select(-Observation) %>%
  mutate(
    Gender = as.factor(Gender)
    )

#combined <- rbind(linregtrain1, linregtest1)
resultsLinReg <- lm(Choice ~ ., data = linregtrain1)
summary(resultsLinReg)
```

```{r}
predict(resultsLinReg, linregtest1, type = 'response') %>% summary()
predict(resultsLinReg, linregtest1, type = 'response') %>% head()
```
Linear Regression Conclusion
Linear Regression is not appropriate as seen above. Results do not give a 0, 1 classification and instead attempt to predict a numeric value of Choice. We can not appropriately classify these numeric values to a class of Choice because it does not correspond to a probability, just that it attempts to fit the values of Choice to a non-existent line.

### Logistic Regression

```{r}
set.seed(321)
logfit <- step(glm(Choice ~ ., 
                   data = train1, family = binomial), 
               direction = "backward", trace = 0)
```

```{r}
summary(logfit)
```

```{r}
car::vif(logfit)
```

```{r}
set.seed(321)
logfit2 <- step(glm(Choice ~ . -Last_purchase, 
                   data = train1, family = binomial), 
               direction = "backward", trace = 0)
```

```{r}
car::vif(logfit2)
```

```{r}
set.seed(321)
logfit3 <- step(glm(Choice ~ . -Last_purchase -First_purchase, 
                   data = train1, family = binomial), 
               direction = "backward", trace = 0)
```

```{r}
car::vif(logfit3)
```

```{r}
predprob_log <- predict(logfit3, newdata = test1, type = "response")
pr_class_log <- ifelse(predprob_log > 0.2, 1, 0)

log_CM_unbal <- caret::confusionMatrix(as.factor(pr_class_log), as.factor(test1$Choice), positive = '1')
log_CM_unbal
```
Logistic regression actually does perform better than the SVM model prior to balancing. This may be the result of the stepwise removal, so we can potentially consider applying the relevant variables to the remaining models when the time comes. Regardless, Sensitivity is still too low to be really valuable, so we'll have to try other methods/ changes.

### Linear Discriminant Analysis (LDA)

```{r}
set.seed(321)
ldafit <- lda(Choice ~ ., data = train1)

ldafit
```

```{r}
pr_class_lda <- predict(ldafit, test1)

lda_CM_unbal <- caret::confusionMatrix(as.factor(pr_class_lda$class), as.factor(test1$Choice), positive = "1")
lda_CM_unbal
```

The LDA model performs remarkably similar to the logit model. So naturally has many of the same advantages/ disadvantages mentioned above.

### Support Vector Machines (SVM)


```{r}
set.seed(321)
form1 <- Choice ~ .

# TAKES A LONG TIME TO RUN!
svmtune <- tune.svm(form1, data = train1, gamma = seq(.01,.1, by = .01), cost = seq(.1, 1, by = .1))
```

```{r}
best_params <- svmtune$best.parameters
print(best_params)
#best parameters: gamma 0.02, cost 0.5
```

```{r}
svmtune$performances
```

```{r}
svmfit <- svm(formula = form1, data = train1, gamma = best_params$gamma, cost = best_params$cost)
summary(svmfit)
```

```{r}
svmpredict <- predict(svmfit, test1, type = 'response')
caret::confusionMatrix(svmpredict, test1$Choice, positive = '1')
```
Sensitivity is terrible, largely due to unbalanced dataset. Balancing data should then improve our performance.

### Balancing Dataset

```{r}
set.seed(321)
trn_art = train1 %>% filter(Choice == '1')
trn_no_art = train1 %>% filter(Choice == '0')

tst_art = test1 %>% filter(Choice == '1')
tst_no_art = test1 %>% filter(Choice == '0')

sample_no_art_trn = sample_n(trn_no_art, nrow(trn_art))
train_bal = rbind(sample_no_art_trn,trn_art)

sample_no_art_tst = sample_n(tst_no_art, nrow(tst_art))
test_bal = rbind(sample_no_art_tst,tst_art)
```

### Logistic Regression (Balanced)

```{r}
set.seed(321)
logfit_bal <- step(glm(Choice ~ ., 
                   data = train_bal, family = binomial), 
               direction = "both", trace = 0)
```

```{r}
summary(logfit_bal)
```

```{r}
predprob_log_bal <- predict(logfit_bal, newdata = test_bal, type = "response")
pr_class_log_bal <- ifelse(predprob_log_bal > 0.5, 1, 0)

log_CM_unbal_bal <- caret::confusionMatrix(as.factor(pr_class_log_bal), as.factor(test_bal$Choice), positive = '1')
log_CM_unbal_bal
```


```{r}
predprob_log_imbal <- predict(logfit_bal, newdata = test1, type = "response")
pr_class_log_imbal <- ifelse(predprob_log_imbal > 0.22, 1, 0)

log_CM_imbal <- caret::confusionMatrix(as.factor(pr_class_log_imbal), as.factor(test1$Choice), positive = '1')
log_CM_imbal
```


### Linear Discriminant Analysis (LDA) (Balanced)

```{r}
set.seed(321)
ldafit_bal <- lda(Choice ~ ., data = train_bal)

ldafit_bal
```

```{r}
pr_class_lda_bal <- predict(ldafit_bal, test_bal)

lda_CM_unbal_bal <- caret::confusionMatrix(as.factor(pr_class_lda_bal$class), as.factor(test_bal$Choice), positive = "1")
lda_CM_unbal_bal
```

```{r}
pr_class_lda_imbal <- predict(ldafit_bal, test1)

lda_CM_imbal <- caret::confusionMatrix(as.factor(pr_class_lda_imbal$class), as.factor(test1$Choice), positive = "1")
lda_CM_imbal
```


### Support Vector Machines (SVM) (Balanced)

```{r}
set.seed(321)
svmtune_bal <- tune.svm(form1, data = train_bal, gamma = seq(.005,.1, by = .005), cost = seq(.1, 1.5, by = .05))
```

```{r}
best_params_bal <- svmtune_bal$best.parameters
print(best_params_bal)
#best parameters: gamma 0.01, cost 1 
```

```{r}
svmfit_bal <- svm(formula = form1, data = train_bal, gamma = best_params_bal$gamma, cost = best_params_bal$cost)
summary(svmfit_bal)
```

```{r}
svmpredict_bal <- predict(svmfit_bal, test_bal, type = 'response')
caret::confusionMatrix(svmpredict_bal, test_bal$Choice, positive = '1')
```

My initial thoughts: Sensitivity is still kind of low, but maybe that's okay? The prediction is about 160 of our 408 observations observations would be likely to buy the book, even though the number who buy would really be closer to 204. Our mailer would go out to those 160, and 34 wouldn't buy (our false positives). So that 34 would be our cost, we would get 126 people to buy, and 78 would be effectively left on the table. This assumes those 78 wouldnt buy without the mailer, and that the 34 who didn't buy wouldn't buy even if they got the mailer. Basically, the problem seems to call more for Specificity anyway.

Revised Commentary: These results are much improved over our original unbalanced SVM results. We see a greater audience of people that we predict would buy the book, resulting a greater Sensitivity score. This comes at some cost to the Specificity but it is an acceptable loss to have a better idea of who would want to purchase the book.

The results above apply our predictions to the test dataset. However, in an attempt to be good stewards of our model and data by applying relevant transformations to both training and test data, we balanced the test set responses. The application of the model to our mailer question does not give us any known information about that audience, so we cannot assume that they would be balanced as well. As a result, we should check our results against an unbalanced dataset to ensure that, if the audience is indeed unbalanced, we are still able to apply our results:

```{r}
svmpredict_imbal <- predict(svmfit_bal, test1, type = 'response')
SVM_CM_imbal <- caret::confusionMatrix(svmpredict_imbal, test1$Choice, positive = '1')
SVM_CM_imbal
#caret::confusionMatrix(svmpredict_imbal, test1$Choice, positive = '1')
```

Note that the test set used for these results is the original test, those which our balanced test set came from. Generally speaking, it's not typically appropriate to utilize test observations more than once, but we will let it slide because we did not make any changes to the model, and to make sure the results are more applicable to the original problem.
Doing this pays off: we see the sensitivity and specificity still perform roughly as well as they did in the balanced dataset. This makes sense, because what really changed is the distribution between the Positive and Negative values of our response variable. Both Sensitivity and Specificity are robust to such changes: the numerators and denominators for each metric do not include results from both classes of the response, only one. This means we can apply the specificity and sensitivity to a hypothetical dataset of random customers.

```{r}
summary_table_svm <- combined %>% group_by(Choice) %>% 
  summarize(percent = n()/nrow(combined),
            newcnt = round(percent * 50000)) %>% as.data.frame() %>% 
  mutate(
    est_targets = ifelse(
      Choice == 0, round(newcnt*(1-SVM_CM_imbal$byClass[["Specificity"]])), round(newcnt*(SVM_CM_imbal$byClass[["Sensitivity"]]))
      ),
    mailercst = est_targets * 0.65,
    purchcst = ifelse(Choice == 1, 15 * 1.45 * est_targets, 0),
    revenue = ifelse(Choice == 1, 31.95 * est_targets, 0),
    profit = revenue - purchcst - mailercst
    )
summary_table_svm
```


```{r}
sum(summary_table_svm$profit)
```

```{r}
summ_tab_fun <- function(base_data, hcount, CM) {
  base_data %>% group_by(Choice) %>% 
  summarize(percent = n()/nrow(base_data),
            newcnt = round(percent * hcount)) %>% as.data.frame() %>% 
  mutate(
    est_targets = ifelse(
      Choice == 0, round(newcnt*(1-CM$byClass[["Specificity"]])), round(newcnt*(CM$byClass[["Sensitivity"]]))
      ),
    mailercst = est_targets * 0.65,
    purchcst = ifelse(Choice == 1, 15 * 1.45 * est_targets, 0),
    revenue = ifelse(Choice == 1, 31.95 * est_targets, 0),
    profit = revenue - purchcst - mailercst
    )
}
```


```{r}
summary_table_log <- summ_tab_fun(combined, 50000, log_CM_unbal)
summary_table_log
```

```{r}
summary_table_lda <- summ_tab_fun(combined, 50000, lda_CM_imbal)
summary_table_lda
```

```{r}
naive_table <- combined %>% group_by(Choice) %>% 
  summarize(percent = n()/nrow(combined),
            newcnt = round(percent * 50000)) %>% as.data.frame() %>% 
  mutate(
    mailercst = newcnt * 0.65,
    purchcst = ifelse(Choice == 1, 15 * 1.45 * newcnt, 0),
    revenue = ifelse(Choice == 1, 31.95 * newcnt, 0),
    profit = revenue - purchcst - mailercst
    )
naive_table
```

```{r}
data.frame(Model = c('Naive', 'LDA', 'Logit', 'SVM'), 
  Profit = c(
    sum(naive_table$profit),
    sum(summary_table_lda$profit),
    sum(summary_table_log$profit),
    sum(summary_table_svm$profit)
    )
  )
```
A little confusing, but this is what I did here:
1. Of the 50,000 people in the audience, estimate the proportion that we anticipate to purchase the book, by using the proportion of people that actually did buy the book across the test and train datasets. The estimate becomes the "newcnt" column
2. Of those that we estimate would or would not buy the book, estimate the number that our model would predict to buy the book. Note there will be people in both the group that buy the book and those that don't that we will predict to buy it. We estimate by taking thoseThis is column est_targets
3. Estimate the mailer cost by multiplying the number of people we predict to buy (est_targets) by $0.65, the cost of the mailer. This is the column mailercst.
4. Estimate the cost to purchase and mail all of the books to those that buy the book. This is done by multiplying 15 by 1.45 (book cost plus allocated overhead), then by the number of people who would by the book, from the column newcnt where Choice == 1. When Choice == 0, there is no book purchase/overhead cost. This is the column purchcst.
5. Estimate the total revenue from all sales of the book. his is done by multiplying 31.95 by the number of people who would by the book from the column newcnt where Choice == 1. When Choice == 0, there is no purchase revenue. This is the column revenue.
6. Profit is then calculated by subtracting the cost of the mailer and the purchase costs from the total revenue. This is the column profit.
7. Summing up the profit column should give us total profit.
8. The same general steps are then applied to the naive method, however in this case we do not need to estimate the number of people we would predict to buy the book since we would be sending the mailer to everyone. Therefore revenue is much higher because we reach all people who would buy the book, but mailer cost and book purchase cost + overhead is much higher as well.

The results are not quite as favorable as we would like. Ideally, we would see that utilizing our model would give us a better ability to target the consumers who would buy the book, therefore saving on mailer, purchase, and overhead costs without sacrificing too much in lost revenue. Because of this we either need to increase revenue (improve sensitivity) or decrease mailer cost (improve specificity). Other models may do this, like logistic regression or LDA, or perhaps simple solutions like variable selection, tuning? Or what if we un-balance it? Like over-sample on those that purchase the book, then Sensitivity could improve at a hopefully less significant cost to Specificity.

Post Logit and LDA:
Adding in the logit and lda models, they both perform better than SVM, but still not as good as the Naive model. However, toying with the threshold for the logit model allows us to increase our specificity, thereby increasing the amount of revenue and profit from those who buy the book. This comes at the expense of specificity, however this only means we send the mailer out to more people. Utilizing the model in this manner, we can find bout half of the total population to send the mailer out to, which in turn captures a greater proportion of customers who would buy the book, enough to earn a greater profit than our naive model does.

After toying with Threshold:
Adjusting the threshold improves the performance of the logistic model significantly. Note that, per our instructors suggestion, I switched back to the unbalance logistic model prior to doing so. I brought the threshold down to .2 by guessing, but it would help to find the optimal. To do that, i'm going to iterate through different threshold values and see what gives the best profit.


```{r}
thresh <- data.frame(threshold = -0.01, profit = 0)
for (i in seq(0, 1, by = 0.01)) {
  preds <- ifelse(predprob_log >= i, 1, 0)
  CM_for <- caret::confusionMatrix(as.factor(preds), as.factor(test1$Choice), positive = '1')
  summ_for <- summ_tab_fun(combined, 50000, CM_for)
  thresh = rbind(thresh, data.frame(threshold = i, profit = sum(summ_for$profit)))
}

thresh <- thresh %>% filter(threshold >= 0)
thresh
```

```{r}
thresh %>% ggplot(aes(x = threshold, y = profit)) +
  geom_line() +
  geom_point() +
  annotate('text', x = thresh[which(thresh$profit == max(thresh$profit)),]$threshold, y = thresh[which(thresh$profit == max(thresh$profit)),]$profit + 1800, label = paste0('Max Profit: $', thresh[which(thresh$profit == max(thresh$profit)),]$profit), size = 3) +
  annotate('point', x = thresh[which(thresh$profit == max(thresh$profit)),]$threshold, y = thresh[which(thresh$profit == max(thresh$profit)),]$profit, color = 'green', shape = 'diamond', size = 3) +
  theme_minimal() +
  ggtitle('Book Sales Profit Changes by Model Probability Threshold') +
  xlab('Logistic Model Probability Threshold') +
  ylab('Profit from Book Sales')

  
thresh[which(thresh$profit == max(thresh$profit)),]
```

```{r}
predslog_best <- ifelse(predprob_log >= thresh[which(thresh$profit == max(thresh$profit)),]$threshold, 1, 0)
  CMlog_best <- caret::confusionMatrix(as.factor(predslog_best), as.factor(test1$Choice), positive = '1')
CMlog_best
```


