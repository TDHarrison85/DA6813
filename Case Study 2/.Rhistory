mutate(
Choice = as.factor(Choice),
Gender = as.factor(Gender)
)
combined <- rbind(train1, test1)
combined %>% select_if(is.numeric) %>% cor() %>% corrplot(method = 'number')
combined %>% mutate(
Gender = ifelse(Gender == 0, 'Female', 'Male'),
Choice = ifelse(Choice == 0, 'Non-purchase', 'purchase')
) %>%
ggplot(aes(x = Gender, fill = Choice)) +
geom_bar() + ggtitle('Gender vs Purchase')
combox <- lapply(colnames(select_if(combined, is.numeric)),
function(col) {
ggplot(combined,
aes(y = .data[[col]], x = .data$Choice)) + geom_boxplot() + ggtitle(col)
}
)
combox[[1]] + combox[[2]] + combox[[3]]
combox[[4]] + combox[[5]] + combox[[6]]
combox[[7]] + combox[[8]] + combox[[9]]
combbar <- lapply(colnames(select_if(combined, startsWith(names(combined), 'P_'))),
function(col) {
ggplot(combined,
aes(x = .data[[col]], fill = .data$Choice)) + geom_bar(position = 'dodge') +
ggtitle(col) +
theme(legend.position = c(0.8,0.8), legend.background = element_blank())
}
)
combbar[[1]] + combbar[[2]]
combbar[[3]] + combbar[[4]]
combbar[[5]]
combined %>% group_by(Choice, P_Art) %>%
summarize(cnt = n()) %>% pivot_wider(id_cols = P_Art, names_from = Choice, values_from = cnt)
set.seed(321)
form1 <- Choice ~ .
svmtune <- tune.svm(form1, data = train1, gamma = seq(.01,.1, by = .01), cost = seq(.1, 1, by = .1))
set.seed(321)
form1 <- Choice ~ .
resultsLM= lm(form1, data = train1)
predLM = predict(resultsLM, newdata=test1)
mean((predLM - test1$Choice)^2)
set.seed(321)
form1 <- Choice ~ .
resultsLM= lm(Choice ~.), data = train1)
set.seed(321)
form1 <- Choice ~ .
resultsLM= lm(Choice ~ ., data = train1)
predLM = predict(resultsLM, newdata=test1)
mean((predLM - train1$Choice)^2)
set.seed(321)
form1 <- Choice ~ .
resultsLM= lm(Choice ~ ., data = train1)
summary(resultsLM)
set.seed(321)
form1 <- Choice ~ .
resultsLM<- lm(Choice ~ ., data = train1)
summary(resultsLM)
set.seed(321)
form1 <- Choice ~ .
resultsLM <- lm(Choice ~ ., data = train1)
summary(resultsLM)
View(resultsLM)
resultsLM <- lm(Choice ~ ., data = train1)
summary(resultsLM)
View(test1)
lmtrain1$Choice <- as.numeric(as.character(train1$Choice))
#Creating data frames that don't make Choice a factor only to run Linear Regression. This is done to show it isn't the appropriate model.
linregtrain1 <- raw_train %>%
select(-Observation) %>%
mutate(
Gender = as.factor(Gender)
)
linreg1 <- raw_test %>%
select(-Observation) %>%
mutate(
Gender = as.factor(Gender)
)
combined <- rbind(linregtrain1, linregtest1)
#Creating data frames that don't make Choice a factor only to run Linear Regression. This is done to show it isn't the appropriate model.
linregtrain1 <- raw_train %>%
select(-Observation) %>%
mutate(
Gender = as.factor(Gender)
)
linregtest1 <- raw_test %>%
select(-Observation) %>%
mutate(
Gender = as.factor(Gender)
)
combined <- rbind(linregtrain1, linregtest1)
resultsLinReg <- lm(Choice ~ ., data = linregtrain1)
summary(resultsLinReg)
pacman::p_load(MASS, tidyverse, e1071, here, readxl, skimr, corrplot, patchwork)
raw_train <- read_xlsx(here('Case Study 2', 'BBBC-Train.xlsx'))
raw_test <- read_xlsx(here('Case Study 2', 'BBBC-Test.xlsx'))
str(raw_train)
anyNA(raw_train)
skim(raw_train)
train1 <- raw_train %>%
select(-Observation) %>%
mutate(
Choice = as.factor(Choice),
Gender = as.factor(Gender)
)
test1 <- raw_test %>%
select(-Observation) %>%
mutate(
Choice = as.factor(Choice),
Gender = as.factor(Gender)
)
combined <- rbind(train1, test1)
combined %>% select_if(is.numeric) %>% cor() %>% corrplot(method = 'number')
combined %>% mutate(
Gender = ifelse(Gender == 0, 'Female', 'Male'),
Choice = ifelse(Choice == 0, 'Non-purchase', 'purchase')
) %>%
ggplot(aes(x = Gender, fill = Choice)) +
geom_bar() + ggtitle('Gender vs Purchase')
combox <- lapply(colnames(select_if(combined, is.numeric)),
function(col) {
ggplot(combined,
aes(y = .data[[col]], x = .data$Choice)) + geom_boxplot() + ggtitle(col)
}
)
combox[[1]] + combox[[2]]
combox[[3]] + combox[[4]]
combbar <- lapply(colnames(select_if(combined, startsWith(names(combined), 'P_'))),
function(col) {
ggplot(combined,
aes(x = .data[[col]], fill = .data$Choice)) + geom_bar(position = 'dodge') +
ggtitle(col) +
theme(legend.position = c(0.8,0.8), legend.background = element_blank())
}
)
combbar[[1]] + combbar[[2]]
combbar[[3]] + combbar[[4]]
combbar[[5]]
combined %>% group_by(Choice, P_Art) %>%
summarize(cnt = n()) %>% pivot_wider(id_cols = P_Art, names_from = Choice, values_from = cnt)
#Creating data frames that don't make Choice a factor only to run Linear Regression. This is done to show it isn't the appropriate model.
linregtrain1 <- raw_train %>%
select(-Observation) %>%
mutate(
Gender = as.factor(Gender)
)
linregtest1 <- raw_test %>%
select(-Observation) %>%
mutate(
Gender = as.factor(Gender)
)
#combined <- rbind(linregtrain1, linregtest1)
resultsLinReg <- lm(Choice ~ ., data = linregtrain1)
summary(resultsLinReg)
predict(resultsLinReg, linregtest1, type = 'response') %>% summary()
predict(resultsLinReg, linregtest1, type = 'response') %>% head()
set.seed(321)
logfit <- step(glm(Choice ~ .,
data = train1, family = binomial),
direction = "backward", trace = 0)
summary(logfit)
car::vif(logfit)
set.seed(321)
logfit2 <- step(glm(Choice ~ . -Last_purchase,
data = train1, family = binomial),
direction = "backward", trace = 0)
car::vif(logfit2)
set.seed(321)
logfit3 <- step(glm(Choice ~ . -Last_purchase -First_purchase,
data = train1, family = binomial),
direction = "backward", trace = 0)
car::vif(logfit3)
predprob_log <- predict(logfit3, newdata = test1, type = "response")
pr_class_log <- ifelse(predprob_log > 0.2, 1, 0)
log_CM_unbal <- caret::confusionMatrix(as.factor(pr_class_log), as.factor(test1$Choice), positive = '1')
log_CM_unbal
set.seed(321)
ldafit <- lda(Choice ~ ., data = train1)
ldafit
set.seed(321)
form1 <- Choice ~ .
# TAKES A LONG TIME TO RUN!
svmtune <- tune.svm(form1, data = train1, gamma = seq(.01,.1, by = .01), cost = seq(.1, 1, by = .1))
pacman::p_load(MASS, tidyverse, e1071, here, readxl, skimr, corrplot, patchwork)
raw_train <- read_xlsx(here('Case Study 2', 'BBBC-Train.xlsx'))
raw_test <- read_xlsx(here('Case Study 2', 'BBBC-Test.xlsx'))
str(raw_train)
anyNA(raw_train)
skim(raw_train)
train1 <- raw_train %>%
select(-Observation) %>%
mutate(
Choice = as.factor(Choice),
Gender = as.factor(Gender)
)
test1 <- raw_test %>%
select(-Observation) %>%
mutate(
Choice = as.factor(Choice),
Gender = as.factor(Gender)
)
combined <- rbind(train1, test1)
combined %>% select_if(is.numeric) %>% cor() %>% corrplot(method = 'number')
combined %>% mutate(
Gender = ifelse(Gender == 0, 'Female', 'Male'),
Choice = ifelse(Choice == 0, 'Non-purchase', 'purchase')
) %>%
ggplot(aes(x = Gender, fill = Choice)) +
geom_bar() + ggtitle('Gender vs Purchase')
combox <- lapply(colnames(select_if(combined, is.numeric)),
function(col) {
ggplot(combined,
aes(y = .data[[col]], x = .data$Choice)) + geom_boxplot() + ggtitle(col)
}
)
combox[[1]] + combox[[2]]
combox[[3]] + combox[[4]]
combbar <- lapply(colnames(select_if(combined, startsWith(names(combined), 'P_'))),
function(col) {
ggplot(combined,
aes(x = .data[[col]], fill = .data$Choice)) + geom_bar(position = 'dodge') +
ggtitle(col) +
theme(legend.position = c(0.8,0.8), legend.background = element_blank())
}
)
combbar[[1]] + combbar[[2]]
combbar[[3]] + combbar[[4]]
combbar[[5]]
combined %>% group_by(Choice, P_Art) %>%
summarize(cnt = n()) %>% pivot_wider(id_cols = P_Art, names_from = Choice, values_from = cnt)
#Creating data frames that don't make Choice a factor only to run Linear Regression. This is done to show it isn't the appropriate model.
linregtrain1 <- raw_train %>%
select(-Observation) %>%
mutate(
Gender = as.factor(Gender)
)
linregtest1 <- raw_test %>%
select(-Observation) %>%
mutate(
Gender = as.factor(Gender)
)
#combined <- rbind(linregtrain1, linregtest1)
resultsLinReg <- lm(Choice ~ ., data = linregtrain1)
summary(resultsLinReg)
predict(resultsLinReg, linregtest1, type = 'response') %>% summary()
predict(resultsLinReg, linregtest1, type = 'response') %>% head()
set.seed(321)
logfit <- step(glm(Choice ~ .,
data = train1, family = binomial),
direction = "backward", trace = 0)
summary(logfit)
car::vif(logfit)
set.seed(321)
logfit2 <- step(glm(Choice ~ . -Last_purchase,
data = train1, family = binomial),
direction = "backward", trace = 0)
car::vif(logfit2)
set.seed(321)
logfit3 <- step(glm(Choice ~ . -Last_purchase -First_purchase,
data = train1, family = binomial),
direction = "backward", trace = 0)
car::vif(logfit3)
predprob_log <- predict(logfit3, newdata = test1, type = "response")
pr_class_log <- ifelse(predprob_log > 0.2, 1, 0)
log_CM_unbal <- caret::confusionMatrix(as.factor(pr_class_log), as.factor(test1$Choice), positive = '1')
log_CM_unbal
set.seed(321)
ldafit <- lda(Choice ~ ., data = train1)
ldafit
pr_class_lda <- predict(ldafit, test1)
lda_CM_unbal <- caret::confusionMatrix(as.factor(pr_class_lda$class), as.factor(test1$Choice), positive = "1")
lda_CM_unbal
set.seed(321)
form1 <- Choice ~ .
# TAKES A LONG TIME TO RUN!
svmtune <- tune.svm(form1, data = train1, gamma = seq(.01,.1, by = .01), cost = seq(.1, 1, by = .1))
best_params <- svmtune$best.parameters
print(best_params)
#best parameters: gamma 0.02, cost 0.5
svmtune$performances
svmfit <- svm(formula = form1, data = train1, gamma = best_params$gamma, cost = best_params$cost)
summary(svmfit)
svmpredict <- predict(svmfit, test1, type = 'response')
caret::confusionMatrix(svmpredict, test1$Choice, positive = '1')
set.seed(321)
trn_art = train1 %>% filter(Choice == '1')
trn_no_art = train1 %>% filter(Choice == '0')
tst_art = test1 %>% filter(Choice == '1')
tst_no_art = test1 %>% filter(Choice == '0')
sample_no_art_trn = sample_n(trn_no_art, nrow(trn_art))
train_bal = rbind(sample_no_art_trn,trn_art)
sample_no_art_tst = sample_n(tst_no_art, nrow(tst_art))
test_bal = rbind(sample_no_art_tst,tst_art)
set.seed(321)
logfit_bal <- step(glm(Choice ~ .,
data = train_bal, family = binomial),
direction = "both", trace = 0)
summary(logfit_bal)
predprob_log_bal <- predict(logfit_bal, newdata = test_bal, type = "response")
pr_class_log_bal <- ifelse(predprob_log_bal > 0.5, 1, 0)
log_CM_unbal_bal <- caret::confusionMatrix(as.factor(pr_class_log_bal), as.factor(test_bal$Choice), positive = '1')
log_CM_unbal_bal
predprob_log_imbal <- predict(logfit_bal, newdata = test1, type = "response")
pr_class_log_imbal <- ifelse(predprob_log_imbal > 0.22, 1, 0)
log_CM_imbal <- caret::confusionMatrix(as.factor(pr_class_log_imbal), as.factor(test1$Choice), positive = '1')
log_CM_imbal
set.seed(321)
ldafit_bal <- lda(Choice ~ ., data = train_bal)
ldafit_bal
pr_class_lda_bal <- predict(ldafit_bal, test_bal)
lda_CM_unbal_bal <- caret::confusionMatrix(as.factor(pr_class_lda_bal$class), as.factor(test_bal$Choice), positive = "1")
lda_CM_unbal_bal
pr_class_lda_imbal <- predict(ldafit_bal, test1)
lda_CM_imbal <- caret::confusionMatrix(as.factor(pr_class_lda_imbal$class), as.factor(test1$Choice), positive = "1")
lda_CM_imbal
set.seed(321)
svmtune_bal <- tune.svm(form1, data = train_bal, gamma = seq(.005,.1, by = .005), cost = seq(.1, 1.5, by = .05))
best_params_bal <- svmtune_bal$best.parameters
print(best_params_bal)
#best parameters: gamma 0.01, cost 1
svmfit_bal <- svm(formula = form1, data = train_bal, gamma = best_params_bal$gamma, cost = best_params_bal$cost)
summary(svmfit_bal)
svmpredict_bal <- predict(svmfit_bal, test_bal, type = 'response')
caret::confusionMatrix(svmpredict_bal, test_bal$Choice, positive = '1')
svmpredict_imbal <- predict(svmfit_bal, test1, type = 'response')
SVM_CM_imbal <- caret::confusionMatrix(svmpredict_imbal, test1$Choice, positive = '1')
SVM_CM_imbal
#caret::confusionMatrix(svmpredict_imbal, test1$Choice, positive = '1')
summary_table_svm <- combined %>% group_by(Choice) %>%
summarize(percent = n()/nrow(combined),
newcnt = round(percent * 50000)) %>% as.data.frame() %>%
mutate(
est_targets = ifelse(
Choice == 0, round(newcnt*(1-SVM_CM_imbal$byClass[["Specificity"]])), round(newcnt*(SVM_CM_imbal$byClass[["Sensitivity"]]))
),
mailercst = est_targets * 0.65,
purchcst = ifelse(Choice == 1, 15 * 1.45 * est_targets, 0),
revenue = ifelse(Choice == 1, 31.95 * est_targets, 0),
profit = revenue - purchcst - mailercst
)
summary_table_svm
sum(summary_table_svm$profit)
summ_tab_fun <- function(base_data, hcount, CM) {
base_data %>% group_by(Choice) %>%
summarize(percent = n()/nrow(base_data),
newcnt = round(percent * hcount)) %>% as.data.frame() %>%
mutate(
est_targets = ifelse(
Choice == 0, round(newcnt*(1-CM$byClass[["Specificity"]])), round(newcnt*(CM$byClass[["Sensitivity"]]))
),
mailercst = est_targets * 0.65,
purchcst = ifelse(Choice == 1, 15 * 1.45 * est_targets, 0),
revenue = ifelse(Choice == 1, 31.95 * est_targets, 0),
profit = revenue - purchcst - mailercst
)
}
summary_table_log <- summ_tab_fun(combined, 50000, log_CM_unbal)
summary_table_log
summary_table_lda <- summ_tab_fun(combined, 50000, lda_CM_imbal)
summary_table_lda
naive_table <- combined %>% group_by(Choice) %>%
summarize(percent = n()/nrow(combined),
newcnt = round(percent * 50000)) %>% as.data.frame() %>%
mutate(
mailercst = newcnt * 0.65,
purchcst = ifelse(Choice == 1, 15 * 1.45 * newcnt, 0),
revenue = ifelse(Choice == 1, 31.95 * newcnt, 0),
profit = revenue - purchcst - mailercst
)
naive_table
data.frame(Model = c('Naive', 'LDA', 'Logit', 'SVM'),
Profit = c(
sum(naive_table$profit),
sum(summary_table_lda$profit),
sum(summary_table_log$profit),
sum(summary_table_svm$profit)
)
)
thresh <- data.frame(threshold = -0.01, profit = 0)
for (i in seq(0, 1, by = 0.01)) {
preds <- ifelse(predprob_log >= i, 1, 0)
CM_for <- caret::confusionMatrix(as.factor(preds), as.factor(test1$Choice), positive = '1')
summ_for <- summ_tab_fun(combined, 50000, CM_for)
thresh = rbind(thresh, data.frame(threshold = i, profit = sum(summ_for$profit)))
}
thresh <- thresh %>% filter(threshold >= 0)
thresh
thresh %>% ggplot(aes(x = threshold, y = profit)) +
geom_line() +
geom_point() +
annotate('text', x = thresh[which(thresh$profit == max(thresh$profit)),]$threshold, y = thresh[which(thresh$profit == max(thresh$profit)),]$profit + 1800, label = paste0('Max Profit: $', thresh[which(thresh$profit == max(thresh$profit)),]$profit), size = 3) +
annotate('point', x = thresh[which(thresh$profit == max(thresh$profit)),]$threshold, y = thresh[which(thresh$profit == max(thresh$profit)),]$profit, color = 'green', shape = 'diamond', size = 3) +
theme_minimal() +
ggtitle('Book Sales Profit Changes by Model Probability Threshold') +
xlab('Logistic Model Probability Threshold') +
ylab('Profit from Book Sales')
thresh[which(thresh$profit == max(thresh$profit)),]
predslog_best <- ifelse(predprob_log >= thresh[which(thresh$profit == max(thresh$profit)),]$threshold, 1, 0)
CMlog_best <- caret::confusionMatrix(as.factor(predslog_best), as.factor(test1$Choice), positive = '1')
CMlog_best
dim(raw_train)
dim(raw_test)
pacman::p_load(MASS, tidyverse, here, car, corrplot, skimr, caret, ggplot2, dplyr, tidyr, ROCR,knitr)
df1 <- read.csv(here('Case Study 1','bank-additional.csv'), sep = ';')
df2 <- df1 %>% mutate_if(is.character, as.factor)
df3 <- df2 %>% mutate(
pcontact = ifelse(pdays == 999, 0, 1),
y = ifelse(y == 'yes',1,0)
) %>% select(!pdays)
# Transforming the dataframe to long format
df_long <- df3 %>%
pivot_longer(cols = c(age, campaign, emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, nr.employed),
names_to = "variable", values_to = "value")
# Creating the boxplots for all variables
ggplot(df_long, aes(x = as.factor(y), y = value)) +
geom_boxplot() +
facet_wrap(~variable, scales = "free") +
labs(x = "y", y = "Value") +
theme_minimal()
# Convert 'previous' and 'pcontact' to factors
dflong2 <- df3 %>% mutate(
previous = as.factor(previous),
pcontact = as.factor(pcontact)
)
# Transforming the dataframe to long format
df_long2 <- dflong2 %>%
pivot_longer(cols = c(job, marital, education, default, housing, loan, contact, month, day_of_week, poutcome, previous, pcontact),
names_to = "variable", values_to = "value")
# Creating bar plots for all categorical variables
ggplot(df_long2, aes(fill = as.factor(y), x = value)) +
geom_bar(position = "dodge") +
facet_wrap(~variable, scales = "free_x", nrow = 3) +  # Set nrow for larger facet grid
labs(x = "Variable", y = "Count", fill = "y") +
ylim(0, 4000) +  # Set y-axis limits from 0 to 4000
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1, size = 8),  # Adjust text angle and size
strip.text = element_text(size = 10)  # Increase facet label size
)
cor1 <- df3 %>% select_if(is.numeric) %>% cor()
corrplot(cor1)
skim(combined)
combined %>% select_if(is.numeric) %>% cor() %>% corrplot(method = 'number')
combined %>% select_if(is.numeric) %>% cor() %>% corrplot(method = 'number')
df4 <- df3 %>% dplyr::select(!duration)
# Split data into training and testing samples
# Setting seed locks the random number generator.
set.seed(321)
trn_part <- sample(nrow(df4),0.8*nrow(df4),replace = F) # Setting training sample to be 80% of the data
dftrain <- df4[trn_part,]
dftest <- df4[-trn_part,]
## Resample with more balanced data
df_sub = df4 %>% filter(y == 1)
df_no_sub = df4 %>% filter(y == 0)
sample_no_sub = sample_n(df_no_sub, nrow(df_sub))
df_bal = rbind(sample_no_sub,df_sub)
# Split data into training and testing balanced samples
set.seed(321)
tr_ind_bal <- sample(nrow(df_bal), 0.8 * nrow(df_bal), replace = FALSE) # Setting training sample to be 80% of the balanced data
dftrain_bal <- df_bal[tr_ind_bal, ]
dftest_bal <- df_bal[-tr_ind_bal, ]
# Stepwise backward elimination (suppress intermediate output)
m7.log <- step(glm(as.factor(y) ~ . - loan - emp.var.rate - euribor3m,
data = dftrain_bal, family = binomial),
direction = "backward", trace = 0)
# Predictions using the final model
predprob_final <- predict(m7.log, newdata = dftest_bal, type = "response")
pr_class_final <- ifelse(predprob_final > 0.5, 1, 0)  # You can adjust the threshold as needed
predprob2_log_bal <- predict(m7.log, newdata = dftest_bal, type = "response")  # Predict probabilities for the test set
predclass2_log_bal <- ifelse(predprob2_log_bal >= 0.5, 1, 0)  # Classify based on the threshold
# Confusion matrix for the final model
conf_matrix <- caret::confusionMatrix(as.factor(pr_class_final), as.factor(dftest_bal$y), positive = '1')
# Extract confusion matrix, accuracy, sensitivity, and specificity
cm1_table <- as.data.frame.matrix(conf_matrix$table)
accuracy <- round(conf_matrix$overall['Accuracy'], 4)
sensitivity <- round(conf_matrix$byClass['Sensitivity'], 4)
specificity <- round(conf_matrix$byClass['Specificity'], 4)
# Create a data frame for the results
results1_df <- data.frame(
Value = c(accuracy, sensitivity, specificity)
)
# Display the AIC at each step
kable(m7.log$anova, format = "pipe", align = "c", caption = "AIC at Each Step")
# Print Confusion Matrix and Model Results
kable(cm1_table, format = "pipe", align = "c", caption = "Confusion Matrix Non-Optimal Value")
kable(results1_df, format = "pipe", align = "c", col.names = c("Metric", "Value"), caption = "Model Results Non-Optimal Value")
set.seed(321)
logfit_bal <- step(glm(Choice ~ .,
data = train_bal, family = binomial),
direction = "both", trace = 0)
svmpredict_imbal <- predict(svmfit_bal, test1, type = 'response')
SVM_CM_imbal <- caret::confusionMatrix(svmpredict_imbal, test1$Choice, positive = '1')
SVM_CM_imbal
#caret::confusionMatrix(svmpredict_imbal, test1$Choice, positive = '1')
data.frame(Model = c('Naive', 'LDA', 'Logit', 'SVM'),
Profit = c(
sum(naive_table$profit),
sum(summary_table_lda$profit),
sum(summary_table_log$profit),
sum(summary_table_svm$profit)
)
)
thresh %>% ggplot(aes(x = threshold, y = profit)) +
geom_line() +
geom_point() +
annotate('text', x = thresh[which(thresh$profit == max(thresh$profit)),]$threshold, y = thresh[which(thresh$profit == max(thresh$profit)),]$profit + 1800, label = paste0('Max Profit: $', thresh[which(thresh$profit == max(thresh$profit)),]$profit), size = 3) +
annotate('point', x = thresh[which(thresh$profit == max(thresh$profit)),]$threshold, y = thresh[which(thresh$profit == max(thresh$profit)),]$profit, color = 'green', shape = 'diamond', size = 3) +
theme_minimal() +
ggtitle('Book Sales Profit Changes by Model Probability Threshold') +
xlab('Logistic Model Probability Threshold') +
ylab('Profit from Book Sales')
thresh[which(thresh$profit == max(thresh$profit)),]
