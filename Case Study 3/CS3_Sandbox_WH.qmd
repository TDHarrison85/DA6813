---
title: "Case Study 3 Sandbox"
format: html
---

## Case Study 3 Sandbox

```{r setup}
pacman::p_load(MASS, tidyverse, here, skimr, rpart, dplyr, VIM, corrplot, car, quantmod, ggplot2, tree, e1071)
select <- dplyr::select
```

```{r}
raw_index_data <- read.csv(here('Case Study 3', 'dow_jones_index.data'))
raw_index_names <- read.csv(here('Case Study 3', 'dow_jones_index.names'))
SP500_raw <- read.csv(here('Case Study 3', 'SP500.csv'), sep = ',', fileEncoding = 'latin1')
```


```{r}
str(raw_index_data)
```
```{r}
skim(raw_index_data)
```


```{r}
anyNA(raw_index_data)
```
#It appears we have missing values, We will need to address this in some of our next steps but first we'll divide the data into train and test

```{r}
missing_values <- colSums(is.na(raw_index_data))

missing_values
```
#Splitting data into training (Q1) and testing (Q2)

```{r}

train_data <- subset(raw_index_data, quarter == 1)  # Training data (Q1: Jan-Mar)
test_data <- subset(raw_index_data, quarter == 2)   # Testing data (Q2: Apr-Jun)

```

#Running correlation check among the numeric variables

```{r}

cor_matrix <- cor(train_data[, sapply(train_data, is.numeric)], use = "complete.obs")

cor_matrix
```
#There appears to be correlation between 'volume' and keep 'previous_weeks_volume'-these two had >.8 (high correlation). Which one would be important to keep for predicting our target variable [percent_change_next_weeks_price]?

#I've decided to parse out the data variables I believe will be most useful moving forward

```{r}

selected_vars <- c('date', 'stock', "percent_change_next_weeks_price", "percent_change_price", 
                   "percent_change_volume_over_last_wk", "previous_weeks_volume", 
                   "days_to_next_dividend", "percent_return_next_dividend", 
                   "open", "high", "low", "close")

train_data_filtered <- train_data[,selected_vars]


test_data_filtered <- test_data[,selected_vars]
  

```

No issue with the variables selected except no date or stock variable?

#Convert necessary columns to numeric

```{r}

train_data_filtered <- train_data_filtered %>%
  mutate(across(c(open, high, low, close), ~ as.numeric(gsub("[$,]", "", .))))  # Clean and convert

test_data_filtered <- test_data_filtered %>%
  mutate(across(c(open, high, low, close), ~ as.numeric(gsub("[$,]", "", .)))) 

```

#impute missing values using KNN in order to maintain integrity of any data patterns

```{r}

# KNN imputation on the training data
train_data_imputed <- kNN(train_data_filtered, variable = c("percent_change_volume_over_last_wk", "previous_weeks_volume"), 
                          k = 5, imp_var = F)  # Adjust k (number of neighbors) as necessary

# Should return 'FALSE' if no missing values
anyNA(train_data_imputed)

```

Variables were selected before but here you went back to your original un-selected training dataset. Adjusted to go with the datset with variable selections. I'm pretty neutral on imputing the data, my only concern here is that we may need to explain reasoning for choosing KNN imputation, and check assumptions for using this method. My original gut check with this is too assume the same volume change, so percent_change_over_last_week would be zero for imputed values, and previous_weeks_volume would equal volume.

#Apply scaling in prep for modeling

```{r}

normalize_min_max <- function(x) {
  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}

train_data_scaled <- train_data_imputed %>%
  mutate(across(where(is.numeric), normalize_min_max))

test_data_scaled <- test_data_filtered %>%
  mutate(across(where(is.numeric), normalize_min_max))

```


#Chage Date variable to true date

```{r}
train_data_scaled <- train_data_scaled %>% 
  mutate(date = parse_date_time(date, '%m/%d/%Y'))

test_data_scaled <- test_data_scaled %>% 
  mutate(date = parse_date_time(date, '%m/%d/%Y'))
```


#Create lagged Variables on scaled data

```{r}
train_data_scaled %>%
  arrange(date) %>%  
  mutate(
    percent_change_price_lag1 = lag(percent_change_price) 
  ) %>% head()
```

Previous method to lag variables does not account for the stock itself. So lag value when ordered by date will pull the values for same date across different stocks.

#Create lagged variables on scaled data version 2

```{r}
train_data_scaled <- train_data_scaled %>%
  group_by(stock) %>% 
  arrange(date) %>%  
  mutate(
    percent_change_price_lag1 = lag(percent_change_price, n = 1) 
  ) %>% ungroup()

test_data_scaled <- test_data_scaled %>%
  group_by(stock) %>% 
  arrange(date) %>%  
  mutate(
    percent_change_price_lag1 = lag(percent_change_price, n = 1) 
  ) %>% ungroup()
```

Group by should solve earlier issue

#Plotting the lagged variables to visualize correlation

```{r}

ggplot(train_data_scaled, aes(x = percent_change_price_lag1, y = percent_change_price)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", color = "blue", se = FALSE) +
  labs(
    title = "Lagged Variable Plot: Percentage Change in Price (t) vs (t-1)",
    x = "Percentage Change in Price (t-1)",
    y = "Percentage Change in Price (t)"
  ) +
  theme_minimal()

```

#Now applying data subset variables to a regression tree, our target variable is [percent_change_next_weeks_price] so we're going to use this regression tree to assess our features importance (which ones are most predictive?)

#wrapper outside of tree model/use for loop == stock name/store rmse/mape into frame

```{r}

tree_model <- rpart(percent_change_next_weeks_price ~ ., data = train_data_scaled, method = "anova")

print(tree_model)
printcp(tree_model)  

plot(tree_model, uniform = TRUE, main = "Regression Tree for Stock Price Prediction")
text(tree_model, use.n = TRUE, all = TRUE, cex = 0.7)

```

#this looks nuts, not sure what to change to make it better


#gonna try pruning, also using Dr. Roy methods for creating tree

```{r}
set.seed(123)

lagtree <- tree(percent_change_next_weeks_price ~ ., data = train_data_scaled)

# If needed, pruning can be performed by specifying the "best" argument 
cv.lag <- cv.tree(lagtree)
best_size <- cv.lag$size[which.min(cv.lag$dev)]
prune.lag <- prune.tree(lagtree, best = best_size)
summary(prune.lag)
print(best_size)


# Get predictions on the test data 
dtpreds <- predict(lagtree, newdata = test_data_scaled)
testpreds <- data.frame(date = test_data_scaled$date,
                        stock = test_data_scaled$stock,
                        response = test_data_scaled$percent_change_next_weeks_price,
                        dt = dtpreds)

mse_dt = mean((testpreds$response - testpreds$dt)^2)
mae_dt = mean(abs(testpreds$response - testpreds$dt))
me_dt = mean(testpreds$response - testpreds$dt)
mape_dt = mean(abs(testpreds$response - testpreds$dt)*100/testpreds$response)

testperfs <- data.frame(dt = c(mse_dt, mae_dt, me_dt, mape_dt)
                        ) %>% t()
colnames(testperfs) <- c('mse', 'mae', 'me', 'mape')
print('')
print(testperfs)
```

after pruning, best size given is 1, which likely means decision tree won't work out well anyway. but opting for no pruning for preds, we can always change this

Mape will always be Inf because of zero values for response. Probably don't even need to include it, but i have it anyway in case we need to talk about why we don't have it.

# Linear model

```{r}
lmfit1 <- step(lm(percent_change_next_weeks_price ~ . - percent_change_price_lag1, data = na.omit(train_data_scaled)), method = 'both')
summary(lmfit1)
lmpreds <- predict(lmfit1, test_data_scaled)

testpreds$lm <- lmpreds

mse_lm = mean((testpreds$response - testpreds$lm)^2)
mae_lm = mean(abs(testpreds$response - testpreds$lm))
me_lm = mean(testpreds$response - testpreds$lm)
mape_lm = mean(abs(testpreds$response - testpreds$lm)*100/testpreds$response)

testperfs <- rbind(testperfs, lm = c(mse_lm, mae_lm, me_lm, mape_lm))
print(testperfs)
```

I had issues with missing values from the lag variable, so may need to bring this up to the professor later.

```{r}
plot(lmfit1)
```


# SVR

SVR is omitting the rows with the NA for percent_change_price_lag1, so removing that variable here.
```{r}
train_data_scaled <- train_data_scaled[ , names(train_data_scaled) != 'percent_change_price_lag1']
test_data_scaled <- test_data_scaled[ , names(test_data_scaled) != 'percent_change_price_lag1']
```


```{r}
unique(train_data_scaled$stock)
```

Getting idea of parameters without for loop since it wouldn't behave on its own. This was primarily just research and does need to be repeated for other models.
```
form1 <- percent_change_next_weeks_price ~ . - percent_change_price_lag1

svr_tune_lists <- list() 
best_Params_list <- list()

set.seed(123)
svr_tune_lists[['AA']] <- tune.svm(form1, data = filter(train_data_scaled, stock == 'AA')[, names(train_data_scaled) != 'stock'],
                                   gamma = seq(.01,.1, by = .01), cost = seq(2.2, 2.5, by = .01), scale = T
                                   )
print(svr_tune_lists[['AA']])

set.seed(123)
svr_tune_lists[['AXP']] <- tune.svm(form1, data = filter(train_data_scaled, stock == 'AXP')[, names(train_data_scaled) != 'stock'],
                                   gamma = seq(.001,.01, by = .001), cost = seq(9, 10, by = .05), scale = T
                                   )
print(svr_tune_lists[['AXP']])

set.seed(123)
svr_tune_lists[['BA']] <- tune.svm(form1, data = filter(train_data_scaled, stock == 'BA')[, names(train_data_scaled) != 'stock'],
                                   gamma = seq(.01,.1, by = .01), cost = seq(1.5, 2.5, by = .1), scale = T
                                   )
print(svr_tune_lists[['BA']])

set.seed(123)
svr_tune_lists[['BA']] <- tune.svm(form1, data = filter(train_data_scaled, stock == 'BA')[, names(train_data_scaled) != 'stock'],
                                   gamma = seq(.01,.1, by = .01), cost = seq(1.5, 2.5, by = .1), scale = T
                                   )
print(svr_tune_lists[['BA']])
```

```{r}
form1 <- percent_change_next_weeks_price ~ .

svr_tune_lists <- list() # initiate models list
best_Params_list <- list() # initiate list of best parameters for SVR for each stock

for (i in c(unique(train_data_scaled$stock))) {
  set.seed(123)
  svr_tune_lists[[i]] <-  tune.svm(form1, data = filter(train_data_scaled, stock == i)[, names(train_data_scaled) != 'stock'],
                                   gamma = seq(.005,.1, by = .005), cost = seq(1, 10, by = 1), scale = T
                                   )
  best_Params_list[[i]] <- svr_tune_lists[[i]]$best.parameters
}
```


checks for any models that did not predict because of bad tuning parameters.
```{r}
for (i in c(unique(train_data_scaled$stock))) {
    print(is.na(best_Params_list[[i]]$gamma))
}

```

```{r}
# this is the part to copy for the other models
svr_fit_list <- list()

for (i in c(unique(train_data_scaled$stock))) {
  set.seed(123)
  svr_fit_list[[i]] <-  svm(form1, data = filter(train_data_scaled, stock == i)[, names(train_data_scaled) != 'stock'],
                                   gamma = best_Params_list[[i]]$gamma, cost = best_Params_list[[i]]$cost, scale = T
                                   )
}
```

this is used to print the summary of each model. Useful if you want to look, but the output is huge and blows up the view a bit.
```
for (i in c(unique(train_data_scaled$stock))) {
    print(summary(svr_fit_list[[i]]))
}

```
stores predictions and other identifying variables into a list of dataframes. This is flattened later into one dataframeb, but must start as a list in order to more easily iterate through the stocks.

```{r}
newpreds <- list()
for (i in c(unique(train_data_scaled$stock))) {
  newpreds[[i]] <- test_data_scaled %>% filter(stock == i) %>% select(date, stock, percent_change_next_weeks_price) # first model needs this line, all others can skip
  newpreds[[i]]$svr <- predict(svr_fit_list[[i]], filter(test_data_scaled, stock == i)[, names(test_data_scaled) != 'stock'])
}
```

we do this step only once we have all predictions for all models saved to the list like we do above
```{r}
predsdf <- bind_rows(newpreds, .id = "column_label")
```

gets MSE for just SVR for now.
```{r}
predsdf %>% group_by(stock) %>% 
  summarize(
    SVR = mean((na.omit(percent_change_next_weeks_price) - svr)^2)
  )
```

Overall MSE
```{r}
predsdf %>% 
  summarize(
    SVR = mean((na.omit(percent_change_next_weeks_price) - svr)^2)
  )
```


Questions for Dr. Roy:
1. Create Lag variable before or after test train split? Same for lag plots? Creates loss of observations in test when done after. create after, don't want training data in your test dataset
2. Group by for lag variable calculation? yes
3. Lag variable keeps getting eliminated in step function for lm, should it be? Since we are using percent instead of one of the open or close prices, lag is pretty useless.
4. KNN imputation appropriate for missing values? yes
5. Tree pruning returns 1 node? didn't get to this question, but may be very different once we start fitting models the right way for dt anyway.



# PICKING UP WHERE WILL LEFT OFF FROM HERE. THIS PART ADDS FOR LM - HOLLY

#Have to define the formula when using LM since it doesn't perform the same as SVR when it comes to feature selection

```{r}
# Defining the formula for LM dynamically to use all existing variables except 'percent_change_next_weeks_price'
lm_formula <- as.formula(paste("percent_change_next_weeks_price ~ ."))

lm_fit_list <- list()

for (i in unique(train_data_scaled$stock)) {
  set.seed(123)
  stock_data <- filter(train_data_scaled, stock == i) %>% select(-stock)
  lm_fit_list[[i]] <- lm(lm_formula, data = stock_data)
}

```

#Adding my LM predictions to the 'newpreds' list

```{r}

for (i in unique(test_data_scaled$stock)) {
  newpreds[[i]]$lm_pred <- predict(lm_fit_list[[i]], newdata = filter(test_data_scaled, stock == i))
}

combined_preds_df <- bind_rows(newpreds, .id = "stock")

```


# Decision Trees
```{r}
# Define formula for the decision tree model
tree_formula <- as.formula("percent_change_next_weeks_price ~ .")

# Initialize a list to store the decision tree fits for each stock
tree_fit_list <- list()

# Loop through each unique stock and fit a decision tree model
for (i in unique(test_data_scaled$stock)) {
  set.seed(123)
  
  # Filter data for the current stock, excluding the 'stock' column
  #stock_data_dt <- filter(train_data_scaled_dt, stock == i) %>% select(-stock)
  
  # Fit the decision tree model for the current stock
  tree_fit_list[[i]] <- tree(tree_formula, data = stock_data)
}

# Example: Access the decision tree model for a specific stock, e.g., "AA"
summary(tree_fit_list[["AA"]])  # Provides a summary of the tree for stock "AA"
plot(tree_fit_list[["AA"]])     # Plots the decision tree for stock "AA"
text(tree_fit_list[["AA"]], pretty = 0) # Adds text to the tree plot

# Initialize a list to store predictions
treepreds <- list()

# Loop through each unique stock and get predictions from the decision tree model
for (i in unique(test_data_scaled$stock)) {
  # Make predictions using the decision tree model for the specific stock
  treepreds[[i]] <- filter(test_data_scaled, stock == i)
  newpreds[[i]]$tree_pred <- predict(tree_fit_list[[i]], newdata = treepreds[[i]])
}

# Combine all predictions into a single data frame
combined_preds_df <- bind_rows(newpreds, .id = "stock")

# View the combined predictions
print(combined_preds_df)






```


#Comparing what we have so far with LM, SVR, and Decision Trees using MSE/MAE/MAPE for each model and stock

```{r}

performance_metrics <- combined_preds_df %>%
  group_by(stock) %>%
  summarize(
    RMSE_LM = sqrt(mean((percent_change_next_weeks_price - lm_pred)^2, na.rm = TRUE)),
    #MAE_LM = mean(abs(percent_change_next_weeks_price - lm_pred), na.rm = TRUE),
    #MAPE_LM = mean(abs(percent_change_next_weeks_price - lm_pred) * 100 / abs(percent_change_next_weeks_price), na.rm = TRUE),
    
    RMSE_SVR = sqrt(mean((percent_change_next_weeks_price - svr)^2, na.rm = TRUE)),
    #MAE_SVR = mean(abs(percent_change_next_weeks_price - svr), na.rm = TRUE),
    #MAPE_SVR = mean(abs(percent_change_next_weeks_price - svr) * 100 / abs(percent_change_next_weeks_price), na.rm = TRUE),
    
    RMSE_Tree = sqrt(mean((percent_change_next_weeks_price - tree_pred)^2, na.rm = TRUE)),
    #MAE_Tree = mean(abs(percent_change_next_weeks_price - tree_pred), na.rm = TRUE),
    #MAPE_Tree = mean(abs(percent_change_next_weeks_price - tree_pred) * 100 / abs(percent_change_next_weeks_price), na.rm = TRUE)
  )

print(performance_metrics)

```

Isolated down to RMSE only to make it more consumable. Dr. Roy suggested RMSE or MAPE, MAPE would have problems with models where there is a value of 0 for the actual value.

# Two thought processes for overall RMSE: taking mean across stocks

```{r}
performance_metrics %>% summarize(
  RMSE_LM = mean(RMSE_LM),
  RMSE_SVR = mean(RMSE_SVR),
  RMSE_Tree = mean(RMSE_Tree)
  )
```

# calculating RMSE across all predictions without stock groupings

```{r}
combined_preds_df %>%
  summarize(
    RMSE_LM = sqrt(mean((percent_change_next_weeks_price - lm_pred)^2, na.rm = TRUE)),
    RMSE_SVR = sqrt(mean((percent_change_next_weeks_price - svr)^2, na.rm = TRUE)),
    RMSE_Tree = sqrt(mean((percent_change_next_weeks_price - tree_pred)^2, na.rm = TRUE))
  )
```


#Interpretation of output:
#LM does not seem to be performing well compared to SVR (this could be do to it not handling outliers well?).SVR Outperforms LM in MSE and MAE- has lower MSE and MAE values across most stocks, indicating it may have a better fit to the data than LM. For example, for stock AA, MSE_SVR is 0.0546 compared to MSE_LM of 353.17. This pattern holds for many stocks, showing SVR may be a better choice here.Additionally, MAPE values for LM are significantly higher than those for SVR in most cases, which suggests that SVR’s predictions are relatively closer to actual values as a percentage. Some of the MAPE values for LM are very large (like 5168.89 for AA), which might indicate that LM struggles with certain stocks, potentially due to variability or outliers in the data.



```{r}

# Plotting Actual vs. Predicted for LM and SVR
ggplot(combined_preds_df, aes(x = percent_change_next_weeks_price)) +
  geom_point(aes(y = lm_pred, color = "LM"), alpha = 0.5) +
  geom_point(aes(y = svr, color = "SVR"), alpha = 0.5) +
  geom_point(aes(y = tree_pred, color = "Tree"), alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  labs(
    title = "Actual vs Predicted Values for LM, SVR, and Decision Trees",
    x = "Actual Percent Change Next Week's Price",
    y = "Predicted Percent Change Next Week's Price"
  ) +
  facet_wrap(~ stock) +
  scale_color_manual(name = "Model", values = c("LM" = "blue", "SVR" = "red", "Tree" = 'green')) +
  theme_minimal()

```


Holly Comments/observations:
Regression Tree and Pruning
pruning resulting in only one node suggests that the tree might not capture much complexity in the data, possibly due to:
Limited predictive strength of the features relative to the target
This may be good to consider alternate models, as we will do with Linear Regression (LM) and SVR, to assess if these might capture the patterns better

# CAPM

data obtained from Yahoo finance [here](https://finance.yahoo.com/quote/%5EGSPC/history/?period1=1294358400&period2=1308873600&frequency=1wk)

```{r}
SP500_raw
```

```{r}
SP500 <- SP500_raw %>% mutate(
  Date = parse_date_time(Date, '%d-%b-%y'),
  Date = Date + days(4)
) %>% mutate_at(
  2:7, parse_number
) %>% mutate(
  percent_change = (Close. - Open)/Open
) %>% select(Date, percent_change)
```


Example for calculation

```{r}
train_data_filtered %>% mutate(pct = (close-open)/open) %>% select(pct, percent_change_price) %>% head()
```

```{r}
risk_data <- raw_index_data %>% select(date, stock, percent_change_price) %>% 
  mutate(
    date = parse_date_time(date, '%m/%d/%Y'),
    percent_change_price = percent_change_price/100
  )
```

```{r}
rd_joined <- left_join(risk_data, SP500, join_by(date == Date))
```

```{r}
risk_fit_list <- list()

for (i in unique(rd_joined$stock)) {
  set.seed(123)
  stock_risk <- filter(rd_joined, stock == i)
  risk_fit_list[[i]] <- lm(percent_change_price ~ percent_change, data = stock_risk)
}

```


```{r}
stock_betas <- list()

for (i in unique(rd_joined$stock)) {
  stock_betas[[i]] <- summary(risk_fit_list[[i]])$coefficients[2, 1] %>% as.data.frame()
}

betas_df <- bind_rows(stock_betas, .id = 'stock')
names(betas_df) <- c('stock', 'betas')
print(betas_df)
```

```{r}
Jul1_preds <- combined_preds_df %>% filter(date == max(date)) %>% select(stock, percent_change_next_weeks_price, svr)

risk_pred <- Jul1_preds %>% left_join(betas_df, join_by(stock))
```

risk_pred data frame can be used for the visualization. If for any reason we decide to pick a different model after additional changes or tuning, we can select a different column in chunk above.
I haven't looked too much into picking the best stock, I figure once we have the visualization it will be easier to make that decision. Also, our price predictions and original measurements may not be as interpretable since we normalized the data. For example, stock AA isn't predicted to see a percent change of 0.696 in the next week. SVR still generally needs centering and scaling to work properly, so not sure yet how we unwind this, but just a word of caution for when we give our write up.

#Holly adding Visuals here.....(it will not hurt my feelings if these get deleted or changed)

```{r}

ggplot(risk_pred, aes(x = betas, y = svr)) +
  geom_point(color = "blue", alpha = 0.7) +
  labs(
    title = "Scatter Plot of Predicted Percent Change vs. Betas",
    x = "Betas",
    y = "Predicted Percent Change for Next Week"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5),
    panel.grid.minor = element_blank()
  )

```
#Hopefully put the right variables on the right axis..
WH: Axes look right, percent change is like a function of the risk value, which is represent by Beta. Chart below is a good idea, adding the label. HD looks like good return with only moderate risk, since the value is just above 1 for Beta, it means it is theoretically only slightly more volatile than the market. CVX has the best return while still being less volatile, so depending on what sort of risk a person's stock portfolio is willing to accept that may be the better choice.

```{r}

ggplot(risk_pred, aes(x = betas, y = svr, label = stock)) +
  geom_point(color = "blue", alpha = 0.7) +
  geom_text(vjust = -1, hjust = 1, size = 3) +
  labs(
    title = "Risk-Return Trade-Off by Stock",
    x = "Betas",
    y = "Predicted Percent Change for Next Week"
  ) +
  theme_minimal()

```

#Not at all sure if I'm right about comparing these two fields from risk_pred but I saw we have a prediction for next week and an SVR prediction? Just comparing the two...
WH: percent_change_next_week is the actual value for the week's percent change, not a prediction. In practice we wouldn't have that, but since we are making this model we do have the benefit of hindsight and can see if we got it right. I've changed those across the charts to make sure we are using the SVR prediction instead of the actual value.

```{r}
ggplot(risk_pred, aes(x = stock)) +
  geom_bar(aes(y = percent_change_next_weeks_price, fill = "Actual"), stat = "identity", position = "dodge") +
  geom_bar(aes(y = svr, fill = "SVR"), stat = "identity", position = "dodge", alpha = 0.8) +
  labs(
    title = "Comparison of Actual Percent Change and SVR by Stock",
    x = "Stock",
    y = "Percent Change"
  ) +
  scale_fill_manual(values = c("Actual" = "dodgerblue", "SVR" = "orange")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
This chart highlights what the actual vs predicted relationship would be, i.e. tells us if we pick one stock, are we right about it? We can HD has a slightly lower percent change than we expected, and that CVX is actually slightly higher.
This kind of puts me even more back to a mind set about the normalization. We know which ones might be the best stocks, but we have two relatively good options and we don't know if one of them might even have negative return. If it does, and if we predict as such, we would likely make different choices about what stock. I'm actually wondering if it would be possible for us to "unwind" the normalization algebraically. Basically solving for the original value from the normalization formula from earlier. I'll have to think more about that.