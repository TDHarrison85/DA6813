---
title: "Case Study 3 Sandbox"
format: html
---

## Case Study 3 Sandbox

```{r setup}
pacman::p_load(here, skimr, rpart, dplyr, VIM, corrplot, car)
```

```{r}
raw_index_data <- read.csv(here('Case Study 3', 'dow_jones_index.data'))
raw_index_names <- read.csv(here('Case Study 3', 'dow_jones_index.names'))
```


```{r}
str(raw_index_data)
```
```{r}
skim(raw_index_data)
```


```{r}
anyNA(raw_index_data)
```
#It appears we have missing values, We will need to address this in some of our next steps but first we'll divide the data into train and test

```{r}
missing_values <- colSums(is.na(raw_index_data))

missing_values
```

```{r}
# Split data into training (Q1) and testing (Q2)
train_data <- subset(raw_index_data, quarter == 1)  # Training data (Q1: Jan-Mar)
test_data <- subset(raw_index_data, quarter == 2)   # Testing data (Q2: Apr-Jun)

```

#Now to address the issue of missing volume: The variables: [percent_change_volume_over_last_wk] and [previous_weeks_volume] are missing 30 values. I will need these variables for this project so simply removing them is not an option.Instead, I will use KNN (K-Nearest Neighbors) for imputing the missing values

```{r}
# KNN imputation on the training data
train_data_imputed <- kNN(train_data, variable = c("percent_change_volume_over_last_wk", "previous_weeks_volume"), 
                          k = 5)  # Adjust k (number of neighbors) as necessary

# Should return 'FALSE' if no missing values
anyNA(train_data_imputed)

```
#We'll need to normalize/scale in preparation for trying out different model methods. (3 models to try + need for this step) Decision Trees: don’t require scaling, SVR: requires scaling, Linear Regression: benefits from scaling, but it’s not required.

```{r}
# Normalize the training data using min-max scaling only to numeric columns
normalize_min_max <- function(x) {
  return ((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
}

train_data_scaled <- as.data.frame(lapply(train_data_imputed[, sapply(train_data_imputed, is.numeric)], normalize_min_max))

train_data_scaled <- cbind(train_data_imputed[, !sapply(train_data_imputed, is.numeric)], train_data_scaled)

```


```{r}
# Standardize the training data using Z-score (mean 0, standard deviation 1)
standardize <- function(x) {
  return ((x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE))
}

train_data_standardized <- as.data.frame(lapply(train_data_imputed[, sapply(train_data_imputed, is.numeric)], standardize))

train_data_standardized <- cbind(train_data_imputed[, !sapply(train_data_imputed, is.numeric)], train_data_standardized)

```


#comments - think we should remove 'volume' and keep 'previous_weeks_volume'-these two had >.8 (high correlation). Which one would be important to keep for predicting our target variable [percent_change_next_weeks_price]?

```{r}
# Assessing for any correlation between numeric columns
cor_matrix <- cor(train_data_imputed[, sapply(train_data_imputed, is.numeric)], use = "complete.obs")

cor_matrix
```


```{r}


```








